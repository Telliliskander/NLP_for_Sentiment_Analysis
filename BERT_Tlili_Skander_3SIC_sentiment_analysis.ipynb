{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ6SNYq_tVVC"
      },
      "source": [
        "# Classify text with BERT\n",
        "\n",
        "Fine-tune BERT to perform sentiment analysis on a dataset of plain-text IMDB movie reviews.\n",
        "In addition to training a model, i presented how to preprocess text into an appropriate format.\n",
        "\n",
        "In this notebook:\n",
        "\n",
        "- Load the IMDB dataset\n",
        "- Load a BERT model from TensorFlow Hub\n",
        "- Build your own model by combining BERT with a classifier\n",
        "- Train your own model, fine-tuning BERT as part of that\n",
        "- Save your model and use it to classify sentences\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PHBpLPuQdmK"
      },
      "source": [
        "## About BERT\n",
        "\n",
        "BERT and other Transformer encoder architectures have been wildly successful on a variety of tasks in NLP (natural language processing). They compute vector-space representations of natural language that are suitable for use in deep learning models. The BERT family of models uses the Transformer encoder architecture to process each token of input text in the full context of all tokens before and after, hence the name: Bidirectional Encoder Representations from Transformers.\n",
        "\n",
        "BERT models are usually pre-trained on a large corpus of text, then fine-tuned for specific tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCjmX4zTCkRK"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-YbjCkzw0yU",
        "outputId": "b4f1555a-9d6f-47f5-afa3-6185585e66e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-text==2.13.*\n",
            "  Downloading tensorflow_text-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text==2.13.*) (0.15.0)\n",
            "Collecting tensorflow<2.14,>=2.13.0 (from tensorflow-text==2.13.*)\n",
            "  Downloading tensorflow-2.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (479.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m479.7/479.7 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.59.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (3.9.0)\n",
            "Collecting keras<2.14,>=2.13.1 (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*)\n",
            "  Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (16.0.6)\n",
            "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.16.0)\n",
            "Collecting tensorboard<2.14,>=2.13 (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*)\n",
            "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*)\n",
            "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (0.34.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (0.41.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (2.17.3)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, keras, google-auth-oauthlib, tensorboard, tensorflow, tensorflow-text\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.12.0\n",
            "    Uninstalling tensorflow-estimator-2.12.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.12.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.12.0\n",
            "    Uninstalling keras-2.12.0:\n",
            "      Successfully uninstalled keras-2.12.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 0.4.6\n",
            "    Uninstalling google-auth-oauthlib-0.4.6:\n",
            "      Successfully uninstalled google-auth-oauthlib-0.4.6\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.12.0\n",
            "    Uninstalling tensorboard-2.12.0:\n",
            "      Successfully uninstalled tensorboard-2.12.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.12.0\n",
            "    Uninstalling tensorflow-2.12.0:\n",
            "      Successfully uninstalled tensorflow-2.12.0\n",
            "Successfully installed google-auth-oauthlib-1.0.0 keras-2.13.1 tensorboard-2.13.0 tensorflow-2.13.1 tensorflow-estimator-2.13.0 tensorflow-text-2.13.0\n"
          ]
        }
      ],
      "source": [
        "# A dependency of the preprocessing for BERT inputs\n",
        "!pip install -U \"tensorflow-text==2.13.*\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5w_XlxN1IsRJ"
      },
      "source": [
        "You will use the AdamW optimizer from [tensorflow/models](https://github.com/tensorflow/models)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-P1ZOA0FkVJ",
        "outputId": "984e6852-0c3b-41f2-d788-17f2c8b7cb30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tf-models-official==2.13.*\n",
            "  Downloading tf_models_official-2.13.2-py2.py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.*) (3.0.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.*) (9.4.0)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.*) (0.5.0)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.*) (2.84.0)\n",
            "Collecting immutabledict (from tf-models-official==2.13.*)\n",
            "  Downloading immutabledict-3.0.0-py3-none-any.whl (4.0 kB)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.*) (1.5.16)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.*) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.*) (1.23.5)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.*) (4.1.3)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.*) (4.8.1.78)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.*) (1.5.3)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.*) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.*) (9.0.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.*) (2.0.7)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.*) (6.0.1)\n",
            "Collecting sacrebleu (from tf-models-official==2.13.*)\n",
            "  Downloading sacrebleu-2.3.2-py3-none-any.whl (119 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.*) (1.11.3)\n",
            "Collecting sentencepiece (from tf-models-official==2.13.*)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting seqeval (from tf-models-official==2.13.*)\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.*) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.*) (4.9.3)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.*) (0.15.0)\n",
            "Collecting tensorflow-model-optimization>=0.4.1 (from tf-models-official==2.13.*)\n",
            "  Downloading tensorflow_model_optimization-0.7.5-py2.py3-none-any.whl (241 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-text~=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.*) (2.13.0)\n",
            "Requirement already satisfied: tensorflow~=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.*) (2.13.1)\n",
            "Requirement already satisfied: tf-slim>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.*) (1.1.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.13.*) (0.22.0)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.13.*) (2.17.3)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.13.*) (0.1.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.13.*) (2.11.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.13.*) (4.1.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.13.*) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.13.*) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.13.*) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.13.*) (4.66.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.13.*) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.13.*) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.13.*) (6.1.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.22.0->tf-models-official==2.13.*) (2023.3.post1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (1.59.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (3.9.0)\n",
            "Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (2.13.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (67.7.2)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (2.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (0.34.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official==2.13.*) (0.1.8)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tf-models-official==2.13.*) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tf-models-official==2.13.*) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tf-models-official==2.13.*) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tf-models-official==2.13.*) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tf-models-official==2.13.*) (3.1.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official==2.13.*) (0.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official==2.13.*) (0.3.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official==2.13.*) (4.9)\n",
            "Collecting portalocker (from sacrebleu->tf-models-official==2.13.*)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official==2.13.*) (2023.6.3)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official==2.13.*) (0.9.0)\n",
            "Collecting colorama (from sacrebleu->tf-models-official==2.13.*)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official==2.13.*) (4.9.3)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval->tf-models-official==2.13.*) (1.2.2)\n",
            "Requirement already satisfied: array-record in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.13.*) (0.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.13.*) (8.1.7)\n",
            "Requirement already satisfied: etils[enp,epath,etree]>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.13.*) (1.5.2)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.13.*) (2.3)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.13.*) (1.14.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.13.*) (0.10.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.13.0->tf-models-official==2.13.*) (0.41.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official==2.13.*) (2023.6.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official==2.13.*) (6.1.1)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official==2.13.*) (3.17.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official==2.13.*) (1.61.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=1.6.7->tf-models-official==2.13.*) (5.3.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle>=1.3.9->tf-models-official==2.13.*) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle>=1.3.9->tf-models-official==2.13.*) (3.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.13.*) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.13.*) (3.2.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (3.5.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (3.0.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle>=1.3.9->tf-models-official==2.13.*) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official==2.13.*) (1.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (2.1.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (3.2.2)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=d58a90819f2b4264a4e5e6ac8f70ea3673d4009b16d9738a03475bf119d9a999\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built seqeval\n",
            "Installing collected packages: sentencepiece, tensorflow-model-optimization, portalocker, immutabledict, colorama, sacrebleu, seqeval, tf-models-official\n",
            "Successfully installed colorama-0.4.6 immutabledict-3.0.0 portalocker-2.8.2 sacrebleu-2.3.2 sentencepiece-0.1.99 seqeval-1.2.2 tensorflow-model-optimization-0.7.5 tf-models-official-2.13.2\n"
          ]
        }
      ],
      "source": [
        "!pip install \"tf-models-official==2.13.*\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XgTpm9ZxoN9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from official.nlp import optimization  # to create AdamW optimizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6MugfEgDRpY"
      },
      "source": [
        "## Sentiment analysis\n",
        "\n",
        "This notebook trains a sentiment analysis model to classify movie reviews as *positive* or *negative*, based on the text of the review.\n",
        "\n",
        "You'll use the [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/) that contains the text of 50,000 movie reviews from the [Internet Movie Database](https://www.imdb.com/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vnvd4mrtPHHV"
      },
      "source": [
        "### Download the IMDB dataset\n",
        "\n",
        "Let's download and extract the dataset, then explore the directory structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOdqCMoQDRJL",
        "outputId": "5016fb9c-1ae2-4a29-8126-86b0991fac77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "84125825/84125825 [==============================] - 9s 0us/step\n"
          ]
        }
      ],
      "source": [
        "url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
        "\n",
        "dataset = tf.keras.utils.get_file('aclImdb_v1.tar.gz', url,\n",
        "                                  untar=True, cache_dir='.',\n",
        "                                  cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
        "\n",
        "train_dir = os.path.join(dataset_dir, 'train')\n",
        "\n",
        "# remove unused folders to make it easier to load the data\n",
        "remove_dir = os.path.join(train_dir, 'unsup')\n",
        "shutil.rmtree(remove_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lN9lWCYfPo7b"
      },
      "source": [
        "Next, you will use the `text_dataset_from_directory` utility to create a labeled `tf.data.Dataset`.\n",
        "\n",
        "The IMDB dataset has already been divided into train and test, but it lacks a validation set. Let's create a validation set using an 80:20 split of the training data by using the `validation_split` argument below.\n",
        "\n",
        "Note:  When using the `validation_split` and `subset` arguments, make sure to either specify a random seed, or to pass `shuffle=False`, so that the validation and training splits have no overlap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IwI_2bcIeX8",
        "outputId": "2dd898b0-2129-4fc3-a06a-4d70a2f39839"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "batch_size = 32\n",
        "seed = 42\n",
        "\n",
        "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'aclImdb/train',\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed=seed)\n",
        "\n",
        "class_names = raw_train_ds.class_names\n",
        "train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE) #This line applies caching and prefetching to the training dataset. Caching stores the dataset in memory, which can speed up training. Prefetching overlaps the data preprocessing and model execution, improving overall training speed.\n",
        "\n",
        "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'aclImdb/train',\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    seed=seed)\n",
        "\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "test_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'aclImdb/test',\n",
        "    batch_size=batch_size)\n",
        "\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQ_p0fkVG96Y",
        "outputId": "895f9f85-e5f1-4f42-94a0-67162d4f17c2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<_BatchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_train_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QR3r7uRIIvKq",
        "outputId": "95031778-a8e7-4c98-c803-98c35739c03e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "625"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(raw_train_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpA98qw2KC5L",
        "outputId": "b7ef875e-8488-4dc2-891f-235e28096477"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example: (<tf.Tensor: shape=(32,), dtype=string, numpy=\n",
            "array([b'Silent Night, Deadly Night 5 is the very last of the series, and like part 4, it\\'s unrelated to the first three except by title and the fact that it\\'s a Christmas-themed horror flick.<br /><br />Except to the oblivious, there\\'s some obvious things going on here...Mickey Rooney plays a toymaker named Joe Petto and his creepy son\\'s name is Pino. Ring a bell, anyone? Now, a little boy named Derek heard a knock at the door one evening, and opened it to find a present on the doorstep for him. Even though it said \"don\\'t open till Christmas\", he begins to open it anyway but is stopped by his dad, who scolds him and sends him to bed, and opens the gift himself. Inside is a little red ball that sprouts Santa arms and a head, and proceeds to kill dad. Oops, maybe he should have left well-enough alone. Of course Derek is then traumatized by the incident since he watched it from the stairs, but he doesn\\'t grow up to be some killer Santa, he just stops talking.<br /><br />There\\'s a mysterious stranger lurking around, who seems very interested in the toys that Joe Petto makes. We even see him buying a bunch when Derek\\'s mom takes him to the store to find a gift for him to bring him out of his trauma. And what exactly is this guy doing? Well, we\\'re not sure but he does seem to be taking these toys apart to see what makes them tick. He does keep his landlord from evicting him by promising him to pay him in cash the next day and presents him with a \"Larry the Larvae\" toy for his kid, but of course \"Larry\" is not a good toy and gets out of the box in the car and of course, well, things aren\\'t pretty.<br /><br />Anyway, eventually what\\'s going on with Joe Petto and Pino is of course revealed, and as with the old story, Pino is not a \"real boy\". Pino is probably even more agitated and naughty because he suffers from \"Kenitalia\" (a smooth plastic crotch) so that could account for his evil ways. And the identity of the lurking stranger is revealed too, and there\\'s even kind of a happy ending of sorts. Whee.<br /><br />A step up from part 4, but not much of one. Again, Brian Yuzna is involved, and Screaming Mad George, so some decent special effects, but not enough to make this great. A few leftovers from part 4 are hanging around too, like Clint Howard and Neith Hunter, but that doesn\\'t really make any difference. Anyway, I now have seeing the whole series out of my system. Now if I could get some of it out of my brain. 4 out of 5.',\n",
            "       b\"This Italian film from the '70's is NOT even in the class with Dog Soldiers, The Howling, or even that awful American Werewolf in Paris, BUT...it is fun to watch. I'm talking about watching the lead actress, a stunning blonde, run amok in her birthday suit. We're talking about graphic, complete nudity...it's obvious that she is a real blonde...humma humma humma!! The story is a hoot, the SFX are childish, and the acting (for the most part) stinks. The only redeeming value of this movie is all (and there is a LOT) the nudity & sex scenes. Tame by HBO standards, but still fun to see when you find yourself without a date on Saturday night. OK...HERE'S THE SPOILER...There is NO werewolf (except in the opening scene of the heroine(??)'s ancestor. The girl just imagines that she's a werewolf...in other words, a clinical Lycanthrope.\",\n",
            "       b'Mr Perlman gives a standout performance (as usual). Sadly, he has to struggle with an underwritten script and some nonsensical set pieces.<br /><br />Larsen is in \"Die Hard\" mode complete with singlet and bulging muscles, I\\'m sure he could do better but seems satisfied to grimace and snarl through his part.<br /><br />The lovely Erika is very decorative (even though fully clothed!) and shows some signs of \"getting\" acting at last.<br /><br />SFX are mainly poor CGI and steals from other movies.<br /><br />The shootouts are pitiful - worthy of the A-Team<br /><br />Not even worth seeing for Perlman - AVOID',\n",
            "       b\"I'm a Christian who generally believes in the theology taught in Left Behind. That being said, I think Left Behind is one of the worst films I've seen in some time.<br /><br />To have a good movie, you need to have a well-written screenplay. Left Behind fell woefully short on this. For one thing, it radically deviates from the book. Sometimes this is done to condense a 400-page novel down to a two-hour film, but in this film I saw changes that made no sense whatsoever.<br /><br />Another thing, there is zero character development. When characters in the story get saved (I won't say who), the book makes it clear that it's a long, soul-searching process. In the film it's quick and artificial. The book is written decently enough where people like Rayford Steele, Buck Williams and Hattie Durham seem real, but in the movie scenarios are consistently given the quick treatment without anything substantial. In another scene where one character gets angry about being left behind (again, I won't say who), it seems artificial.<br /><br />I realize as a Christian it's unedifying for me to say I disliked this film, but I can't in a good conscience recommend a film that I feel was horribly done. Perhaps it would've been better to make the first book into 2-3 films. Either way, Christians need to realize that to be taken seriously as filmmakers, we need to start by putting together a film in a quality way. I realize a lot of effort probably went into Left Behind, but that's the way I see it.\",\n",
            "       b'The Forest isn\\'t just your everyday standard slasher/backwoods cannibal fare, it also has an interesting mix of supernatural elements as well. The story is about two couples that hike into the forest on a camping trip. A cave dwelling, cannibalistic woodsmen and the ghosts of his dead wife and two children soon terrorize them. There is something you don\\'t see every slasher. Director Don Jones gets an \"A\" for effort although the film itself falls flat on just about every level, the acting is just simply average except for Jeanette Kelly who plays the dead wife of the woodsman (Michael Brody aka Gary Kent).<br /><br />The film opens with some beautiful shots of a couple hiking through a valley and into a forest. They realize too late that someone is stalking them. They are both dispatched in typical slasher fare. Our killer uses a trusty hunting knife throughout the entire film, except during a flashback when he implements a handsaw, pitchfork and rusty saw blade to dispatch his cheating wife\\'s lover.<br /><br />The Forest has a good story line but the movie just doesn\\'t work along with it I found it pretty boring with simply crappy acting. 4/10',\n",
            "       b'Well, if you like pop/punk, punk, ska, and a tad bit of modern psycho billy, then seeing the live performances are about the only thing worth watching. This movie has tons and tons of band cameos, along with president of Troma, Lloyd Kaufman as a semi-major role, and lots of goofy death scenes. Sounds like it may be good, right? Well, the deaths keep coming, and repeatedly to many different bands of the Warp Tour and the fans at the event. Some of the deaths start of stylish, but then they are recycled over and over, to the point of being completely repetitive. Almost everyone dies of having their head smashed, or intestines being pulled from their stomach. The gore looks as if it was from Andreas Schnaas\\' \"Zombie 90: Extreme Pestilence\"; with this being the \"watered-down type blood\", but now that movie is actually decent, and provides humor-something that this movie terribly lacks. Sure, the movie is made by Doug Sakmann from Troma, it\\'s got great low-budget potential, and it tries...but just too hard. Everything is overly meant to be funny in this movie, and thats what brings it down. Everything tries to be too comic and goofy, by using intentional bad acting, an overuse of pointless deaths, and doing the same thing...over and over. It\\'s basically \"Mulva: Zombie Ass-Kicker\", \"Chairman of the Board\", or any movie you have made with your friends: it\\'s funny to those who made it, and that\\'s about it.<br /><br />Great potential, great idea, great use of effects-but it\\'s the same thing...over and over: A band plays, a band dies, fans die. Everyone dies, blood is sprayed everywhere, the process is repeated.<br /><br />The question is for these types of movies-which is basically \\'bad slap-stick\\'-do they try too hard, or not at all?',\n",
            "       b\"I LOVE the Doodlebops. My son has been watching them for over a year. We went to the Doodlebops concert last year as well as one concert yesterday (connecticut). He LOVES them. The doodlebops do not teach the alphabet or numbers but who cares? are you being serious? the TV isn't suppose to teach your children about numbers or the alphabet. the parents should. Get over it. The Doodlebops actually CAN sing. Deedee has a beautiful voice and in concert you can tell all 3 of them have nice singing voices and do NOT lip sing. Imagine, they dance, jump around and STILL sing. they have talent, the kids love them i even enjoy watching the show. This show is by far the best show on TV for kids. AND a rock band for children. How amazing is that? Why are people saying Chad (rooney) is gay? where did you hear that from? Whether he is or not, He is awesome! Leave him alone. Its not like he or anyone else is promoting homosexuality to our children!\",\n",
            "       b\"I'm a huge Steven Seagal fan. Hell, I probably weigh as much as he does although I don't have the street cred to sport the frizzy-mullet-ponytail. Having stated my own bias and affection for America's favorite corpulent stage and screen hero, it is with a heavy heart that I must declare this to be his worst movie ever. I'm not sure he could make a movie any worse than this.<br /><br />In his defense the major problems with this film seem to occur in post-production. It's painfully obvious that this movie was supposed to have a different storyline. That results in woeful voiceovers in which Steve's voice doesn't nearly sync up with that of the dubbed voice. The editing is pisspoor and overall this starts bad, gets even worse, and by the end you'll wish you had rewatched The Da Vinci Code instead. Yes, it's that bad.<br /><br />After this I don't know what to expect from Steve. My friends still laugh at me for listening to his CDs. Is it time I start checking out some of the Van Damme direct to DVD nutty logs? If you are tempted to watch this movie, rip your eyeballs out and flush them down the toilet. A lifetime of darkness is better than 89 minutes of this.\",\n",
            "       b'Although recognized as the best film treatment of the difficulties of having a house in the country built (or bought) to your specifications, it is not the first, nor the last. In 1940 Jack Benny and Ann Sheridan were the leads in the film version of the comedy GEORGE WASHINGTON SLEPT HERE by George S. Kaufman and Moss Hart. And about fifteen years ago Shelly Long and Tom Hanks had the lead in THE MONEY PIT. The former was about moving into an 18th Century country house that...err, needs work. The latter was about building your dream house - in the late 1980s. Although the two films have their moments, both are not as good as BLANDINGS, which was based on an autobiographical novel of the same name.<br /><br />Jim Blandings and his wife Muriel (Cary Grant and Myrna Loy) are noticing the tight corners of their apartment, which they share with their two daughters Joan and Betsy (Sharyn Moffett and Connie Marshall). Although Blandings has a good income as an advertising executive (in 1948 he is making $15,000.00 a year, which was like making $90,000.00 today), and lives in a luxury apartment - which in the New York City of that day he rents! - he feels they should seek something better. He and Muriel take a drive into the country (Connecticut) and soon find an old ruin that both imagine can be fixed up as that dream house they want.<br /><br />And they both fall into the financial worm hole that buying land and construction can lead to. For one thing, they are so gung ho about the idea of building a home like this they fail to heed warning after warning by their wise, if cynical friend and lawyer Bill Cole (Melvin Douglas, in a nicely sardonic role). For example, Jim buys land from a Connecticut dealer (Ian Wolfe, sucking his chops quietly), with a check before double checking the correct cost for the land in that part of Connecticut. Bill points out he\\'s paid about five or six thousand dollars more for the land than it is worth. There are problems about water supply that both Blandings just never think about, such as hard and soft water - which leads to the Zis - Zis Water softening machine. They find that the designs they have in mind, and have worked out with their architect (Reginald Denny), can\\'t be dropped cheaply at a spur of the moment decision by Muriel to build a little rookery that nobody planned for. <br /><br />The escalating costs of the project are one matter that bedevils Jim. He has been appointed to handle the \"Wham\" account (\"Spam\" had become a popular result of World War II, in that the public started using it as a meat substitute, in the light of it\\'s success with the armed forces). Jim can\\'t get a grip on this (he\\'s not alone - one or two other executives fumbled it before him). He comes up with the following bit of \"poetry\"(?):<br /><br />\"This little piggy went to market,<br /><br />He was pink and as pretty as ham.<br /><br />He smiled in his tracks,<br /><br />As they gave him the ax -<br /><br />He knew he would end up as \"Wham\"!\"<br /><br />His Secretary looks at him as though he needs a straight jacket when he reads that one!<br /><br />Jim also is increasingly suspicious of the attentions of Bill to Muriel, although (in this case) Bill is blameless. But he\\'s always around (Jim keeps forgetting that Bill is the clearheaded one, and that he\\'s keeping Jim and Muriel from making so many mistakes). All three have mishaps, the best being when they get locked in a room in the half constructed house, just as the men have left for the day. They can\\'t open the door, and Jim (in a panic) tries breaking the door down by a make-shift battering ram. He breaks a window, and the door opens by itself.<br /><br />The film works quite satisfactorily, with all of the actors apparently enjoying themselves. It is one film which (despite changing price levels and salary levels) really does not age at all. After all, most Americans dream of owning their own home and always have.<br /><br />A number of years ago a paint company made use of a delightful scene with Myrna Loy and Emory Parnell regarding the paint job Parnell\\'s company has to do on the various rooms. She carefully shows the distinct shades of red, blue, etc. she wants - even giving a polite Parnell a single thread for the right shade of blue. The commercials hinted that the paint company had a wide variety of colors to choose from for your paint job. They proudly called Loy \"Mrs. Blandings\" in the commercials\\' introduction. You can imagine though how the no-nonsense Parnell handles the situation afterward, when Loy leaves him with his paint crew.',\n",
            "       b\"Considering John Doe apparently inspired Kyle XY's creator I was expecting its pilot to be quite interesting. However I probably had too high expectations because I was quite disappointed by it. First they turned the protagonist into a freak who had the crazy idea of showing off his amazing knowledge in front of an audience, in a public area. So after that scene I began to worry that it was just entertainment. But the problem is that it got worse as none of the other characters were properly introduced. They focused too much on John Doe which made the story far less intriguing. I was also slightly disappointed by Dominic Purcell's performance because I found he didn't make a believable John Doe. An other problem was the police story. It really felt like d\\xc3\\xa9j\\xc3\\xa0 vu and it wasn't a pleasant sensation. It leads us to the worst issue in the bunch, the episodic format. I could already see the fillers coming one after an other.<br /><br />So overall I was very disappointed by it and don't recommend it to anyone. Considering how bad it was I better understand now why the show got canceled. In some way I have the impression that it missed its target, developing characters to help the protagonist find his own identity. It's sad because there was potential, like the people he met at the club. The production quality was also quite good and the casting correct. But I'll never know if it got better, probably not, because I don't plan to watch the next episode.\",\n",
            "       b'The movie \"Atlantis: The Lost Empire\" is a shining gem in the rubble of films produced by the Disney Studios recently. Parents who have had to sit through \"The Jungle Book 2\" or even a Pokemon movie will surely appreciate this one.<br /><br />The film is one of few to attempt at an original story; previous feature films were merely re tellings of existing stories. Films such as \"Toy Story\", \"Finding Nemo\", and \"Monsters Inc.\" all do the same, but it must be noted that all were made by Pixar and only distributed by Disney. Recent films from the Disney Studios are mostly released direct to video, and are sequels to an existing successful film. The quality of those films is given way to the profitability. A new era started with \"Atlantis\" following it were \"Mulan\", \"Lilo & Stitch\", and most recently \"Open Range\". The writers have created all original story lines instead of the fairy tales of the past.<br /><br />A good portion of the movie is devoted to the quest to find Atlantis, a task that has captured the imagination of many for hundreds of years. Including that of young Milo Thatch, voiced by Michael J. Fox. Milo is employed by a museum in Washington D.C.. His grandfather was a renowned archaeologist, who had devoted his life to discovering Atlantis. This was seen as a waste by his peers, and they wish Milo to not follow in his footsteps. After failing to convince the museum board of directors to sponsor his expedition, Milo comes home to find a woman in his darkened apartment. She takes him to her employer, a Mr. Whitmore. Whitmore was a close friend of Milo\\'s grandfather, and wishes to send Milo with a team to locate Atlantis. Mr. Whitmore is very wealth and has paid for the best of everything. The crew that is to accompany him is the same as his grandfathers. The journey is filled with many great obstacles to overcome and is great fun to watch. The viewer finds themselves caught up in if they will reach Atlantis. The plot takes an unexpected turn after the discovery Atlantis, not just the discovery of people. It is enough to keep the interest of the older audience.<br /><br />The animators have done a wonderful job in then depth of the animation. The movie is very successful in blending traditional animation with Computer Generated Images. A feat not easily achieved, most audiences are quick to notice the difference in the two. The characters are believably human. There are some nice chase type scenes, with lots of action going on. A few lulls are filled with jokes that the children just may not get.<br /><br />The creativity of the writers really shines through. The culture of Atlantis is richly developed, including an entire language. The film uses references to Atlantis from historical sources, such as Plato. The disappearance of Atlantis from the world is explained. Believable, if by a younger audience, that magic really does exist. The powers of the people of Atlantis are not exactly presented as magic, but can best be described in this way.<br /><br />Although set in 1914 the level of technology used is unrealistic. The voyage is in a submarine very reminiscent of Captain Nemo\\'s nautilus, complete with sub pods that fire torpedoes. The giant diggers are driven by steam boilers so they did try for some era technology. The female characters are empowered in a way that women of the age would not have been, even holding roles in leadership. This is not a bad thing. It gives a good role model for my daughter to look to, rather than an all male cast.<br /><br />One reason this film is a favorite of mine over other Disney films is that there is not one single song, ever. A tradition that began with the first feature film, \"Snow White\", and carried on through to \"The Lion King\", almost every Disney film is full of upbeat songs. This is great and all, what would the Seven Dwarfs be without \"Hi HO!\"? After the millionth time through it\\'d almost be better without, but this one spares the parent. Not once does every single person on the screen suddenly know the words to a song that no one has ever heard before and break out in song. I for one am grateful.<br /><br />The storyline and depth of animation is sure to keep the attention of both parent and child alike. It is a film I am willing to watch again and again with my children.',\n",
            "       b\"Someone must have been seriously joking when they made this film.<br /><br />Firstly, it is an absolute impossibility that this movie was made in 1993. The fashions and music dictate that this is seriously 80's. My guess is that this has sat on the shelf for a long while before some crazed distributer picked it up and released it to a disbelieving world.<br /><br />There is a plot. Kind of. A strange loner meets a random man with a beard who tells him that if he meditates while singing his favourite song he will be able to turn into whomever he chooses. At this point I feel obliged to point out that the loner's favourite song is London Bridge Is Falling Down. Why is this his favourite song? Because he's an idiot. We are only a minute into the film and already the film has reached a monumental level of stupidity. It gets even stupider.<br /><br />The loner is the nostril picker. I can only assume this as there are two scenes in the film where he is seen picking his nose. That clears up the title. He decides to change into a girl so that he can get close to other girls. And kill them. That's more or less it.<br /><br />The acting is universally appalling. Every single performance in this movie sucks. In fact, I would go so far as to say that the acting is of the standard of a pornographic movie. It really is that terrible. The nostril picker appears to the audience as the nostril picker. The characters in the movie see him as the girl he has become through singing London Bridge Is Falling Down. Man, I feel like an idiot even typing this. Anyway, it is kind of strange seeing a middle aged weirdo hanging out with school girls. And not in a good way. There is even an extended montage of scenes where the nostril picker is at school with the girls and a song plays over the top. It is very possibly the worst song ever recorded. I'm not even going to describe it. You'll know it when you hear it. And you'll agree with me.<br /><br />There are some scenes of violence, sure. And there is a Benny Hill style chase sequence involving a transsexual. There is even an immortal bit of dialogue, that may or may not have been taken from Shakespeare or John Milton, where the nostril picker says to a prostitute, 'I've got the cash if you've got the gash'. Lovely, I'm sure you'll agree.<br /><br />Utter nonsense.\",\n",
            "       b'I love horror movies that brings out a real amount of mystery like say \"silent hill\" ( which i found to be quite good, but still, was missing something ) and movies that keeps you guessing, this i thought was one of those movies. At first the movie starts out with some really good suspense and builds up a good starting point for a good horror scene, but after that it just rolls down the hill and from there it only goes faster and faster down. I mentioned silent hill at first for a reason because i can see a lot of \"stolen\" themes from that movie in here.. All in all i would say, watch silent hill instead of this one, its better, its more scary, it has a lot more suspense and also the ending is a lot better.. And best of all, you wont feel ripped off as i did with this one.. This just seems to be one of those \"i like that movie so I\\'m gonna re-make it in my own really bad version\" kinda movie.. Oh and one more thing... Lordi.. in a horror movie... thats like trying to scare a kid with a care bear who has \"hug me and i will love you forever\" written on the stomach of it..',\n",
            "       b'This British film is truly awful, and it\\'s hard to believe that Glenn Ford is in it, although he pretty much sleepwalks through it. The idea of a bomb on a train sounds good...but it turns out this train ends up parked for the majority of the film! No action, no movement, just a static train. The area where the train is parked is evacuated, so it\\'s not like there\\'s any danger to anyone either. In fact, this film could be used in a film class to show how NOT to make a suspense film. True suspense is generated by letting the audience know things that the characters don\\'t, a fact apparently unknown to the director. SPOILER: the train actually has two bombs on it, but we are led to believe there is only one. After the first bomb is defused, it feels as if there is no longer a reason to watch the film any more. But at the last minute, the villain, who has no apparent motivation for his actions, reveals there are two. Nor are we certain WHEN the bombs will go off, so we don\\'t even have a classic \"ticking bomb\" tension sequence. A good 10 minutes or more are spent watching Glenn Ford\\'s French wife thinking about leaving him, and then wondering where he is . She\\'s such an annoying character that we don\\'t care whether she reconciles with him, so when she does, there\\'s nothing emotional about it. Most of the other characters are fairly devoid of personality, and none have any problems or issues. It\\'s only 72 minutes, but it feels long because it\\'s tedious and dull. Don\\'t waste your time.',\n",
            "       b\"When the Chamberlain family is camping near Ayers Rock, Australia, Lindy Chamberlain (Meryl Streep) sees her baby being dragged out of their tent by a dingo and then begins an ordeal that no one should have to experience. For it seems like the dingo story is not believed by the public or the press, and the whole thing turns into a circus. Lindy doesn't help matters either because she won't play to the jury or courtroom, she's only herself, and she's a tough nut to crack, so of course everyone thinks she's guilty because there's a piece of evidence that hasn't come to light. Sam Neill is excellent as Michael Chamberlain, a Seventh-Day adventist pastor, who has doubts about his faith and perhaps about his wife. It's good (or bad) to see that people are just as prejudiced and stupid elsewhere as they are in the States too, because the Australian public doesn't believe the story and the media only fans the flames. Eventually, Lindy is found guilty and sent to prison for a life of hard labor, but years later, a missing piece of evidence shows up and she's freed, but not until after the family's life is basically ruined. A heart-breaking story, very well done, a bit long but well worth seeing. 8 out of 10.\",\n",
            "       b'boring stuff we got here. His 5 minute shorts are better than this. know why? because there only 5 minutes and not 91 minutes or how ever long this is. <br /><br />The plot is kinda... eh.. the last half hour is alright the rest is boring and not funny =( I had my hopes up, the trailer made it look funny but the pace of this movie is pretty slow and sadly not funny. Just plain boring klaymen running into each other and trying to make us laugh.. not working.<br /><br />Maybe next time knox.<br /><br />Maybe re-cutting this movie and adding better scenes would do a lot of healing but for now its just not good.',\n",
            "       b\"Dull haunted house thriller finds an American family moving into a 200 year old house in Japan where a violent murder suicide love triangle occurred. <br /><br />Novel setting is about the only element of interest in this very slow moving horror flick by the director of Motel Hell. The film generates zero suspense and is composed of somewhat choppy scenes that rarely seem to be leading anywhere overall. <br /><br />One obvious example is a fairly early scene where the male lead visits a temple after realizing that his house is haunted as the monk had earlier warned. The monk recounts the history of the house (which the viewer is already familiar with from the opening sequence) and then the film simply cuts away to something else. Earlier the monk had offered to help. Well, where is the help? The family continues to stay in the haunted house as things get worse and worse and no mention of the monk is made until nearly the very end when he turns up again to do what he should have done an hour earlier--try to drive the spirits out of the house, although by this time it's difficult for the viewers to care.<br /><br />There are some (probably) unintentional campy laughs in seeing the American actors at the end become possessed by the Japanese spirits and suddenly start doing bad martial arts, I say probably because the scene is more than a little reminiscent of the chainsaw duel from the same director's Motel Hell which was more obviously meant to be amusing, but on the whole this is a forgettable dud.\",\n",
            "       b\"Stan Laurel and Oliver Hardy had extensive (separate) film careers before they were eventually teamed. For many of Ollie's pre-Stan films, he was billed on screen as Babe Hardy ... and throughout his adult life, Hardy was known to his friends as 'Babe'. While touring postwar Britain with Laurel in a music-hall act for Bernard Delfont, Hardy gave an interview to journalist John McCabe in which he explained the origin of this nickname: early in his acting career, Hardy got a shave from a gay hairdresser who squeezed Hardy's plump cheeks (the ones on his face) and said 'Nice baby!' Hardy's workmates started crying him 'Babe', and the nickname stuck.<br /><br />Although much of Hardy's pre-Laurel work is very interesting -- notably his comedy roles in support of Larry Semon and the Chaplin imitator Billy West -- his teamwork with Billy Ruge (who?) in a series of low-budget shorts for the Vim Comedy Film Company is very dire indeed. Hardy and Ruge were given the screen names Plump and Runt: names which are unpleasant in their own right, but made worse because Ruge (although shorter than Hardy) isn't especially a runt. Seen here, Hardy looks much as he does in his early Hal Roach films with Laurel ... but without the spit curls and the fastidious little moustache.<br /><br />'One Too Many', an absolutely typical Plunt and Runt epic, is direly unfunny ... and its dreichness is made even more conspicuous by the fact that this film has exactly the same premise as 'That's My Wife', one of Laurel and Hardy's most hilarious films. Plump (Hardy) is the star boarder in a rooming-house run by a tall gawky landlady. Runt (Ruge) is the porter. Plump receives a letter from his wealthy uncle John, whose dosh he expects to inherit. His uncle is coming to see him and to meet Plump's wife and baby. There's only one problem: Plump hasn't got a wife and baby. He's been lying to his uncle in order to seem a family man. Now, of course, Plump expects Runt to find him a wife and baby on short notice. Of course, the results are disastrous. It would be nice if those disastrous results were funny, but they aren't. Most of the unfunny humour here is just empty slapstick, with characters settling their arguments by shoving each other into bathtubs.<br /><br />SPOILERS COMING. Vim director Will Louis (who?) shows no instinct for camera framing: the actress who plays the landlady is significantly taller than Hardy, and Louis consistently sets up his shots so that her head is out of frame. This could be funny if done on purpose, but it's merely inept. At one point in this bad comedy, an extremely tasteless gag is looming on the horizon as Runt approaches a black laundress. 'Surely they wouldn't stoop THAT low for a laugh,' I thought. But they do. Runt steals the woman's black infant and tries to fob this off as Plump's progeny.<br /><br />Somehow, Plump acquires an infant's cot, but he still hasn't got a baby. With Uncle John coming up the stairs, Plump conscripts Runt for babyhood. This gag might just possibly have worked with a midget, or even with a truly runt-sized actor such as Chester Conklin, but Billy Ruge is only slightly below average height. Ruge's impersonation of a baby is neither believable nor funny, and Uncle John would have to be a complete moron to fall for it. Amazingly, he does!<br /><br />The most notable aspect of 'One Too Many' is a brief appearance -- apparently her only-ever film appearance -- by Madelyn Saloshin, Oliver Hardy's first wife. The marriage was not a happy one, although Hardy's marital troubles never attained the epic proportions of Stan Laurel's. <br /><br />Only one thing in this movie impressed me. There is a very brief flashback sequence, with Hardy reminiscing about his seaside romance with a bathing beauty. In 1916, there was still not yet a standard film grammar for conveying flashbacks: the one shown here is done gracefully and simply. Too bad this movie has no other merits. 'One Too Many' is definitely one film too many on Oliver Hardy's CV, and I'll rate this movie just one point out of 10. Laurel and Hardy together are definitely much funnier than either of them separately.\",\n",
            "       b'Yes, I\\'m sentimental & schmaltzy!! But this movie (and it\\'s theme song) remain one of my all time greats!! Robert Downey Jr. does such justice to the role of \"Louis Jeffries\" reincarnated and the storyline (although far-fetched) is romantic & makes one believe in happy endings!!',\n",
            "       b'This is one of Joan Crawford\\'s best Talkies. It was the first Gable-Crawford pairing, and made it evident to MGM and to audiences that they were a sizzling team, leading the studio to make seven more films with them as co-stars.<br /><br />The film convincingly depicts the downward slide of a brother and sister who, after their father loses everything in the stock market crash, must fend for themselves and work for a living. Life is hard in the Depression, and soon even their attempts at finding legitimate work prove futile, and they resort to underworld activity. <br /><br />Joan Crawford is excellent as the socialite-turned-moll. She\\'s smart, complex, and believable. She even tempers the theatrical stiffness of the other actors\\' early Talkie acting style. Clark Gable is a diamond-in-the rough, masculine and gruff as the no-nonsense gangster who becomes involved with Crawford\\'s character. The same year he would play a similar and even more successful role opposite Norma Shearer in \"A Free Soul\", securing his position as top male sex symbol at MGM.<br /><br />If you like Crawford in this type of role, don\\'t miss \"Paid\", which she did a year earlier, which is also among her best early Talkie performances.<br /><br />',\n",
            "       b\"An OUR GANG Comedy Short.<br /><br />The Gang coerces Spanky into watching their younger siblings. Caring for these FORGOTTEN BABIES turns out to be quite a chore, leaving the little nipper with no choice but to come up with some ingenious solutions to the baby-sitting problem...<br /><br />Spanky is in his glory in this hilarious little film, arguably his best. Highlight: Spanky's retelling the plot of the TARZAN movie he's recently seen to the audience of infants. Movie mavens will recognize Billy Gilbert's voice in the radio drama.\",\n",
            "       b'I know, I know: it\\'s childish. But I just love this type of movie. A bird that suffered a lot of \"mishaps\" and still hasn\\'t lost his faith in humanity and his sense of humor. What\\'s special about this film is the fact that the main character is Paulie -the parrot- and he\\'s not used as a boost to some hotshot human actor. Furthermore I like the storyline: Paulie tells his lifestory to a cleaner at the point he hit rock bottom. (By the way: Jay Mohr\\'s voice almost sounds like Joe Pesci\\'s!). And Cheech Marin of course, the man IS humor to me. Ever since I saw \"Up in Smoke\" I have appreciated his naive way of performing, making a simple situation a hilarious one.. can\\'t help myself.',\n",
            "       b'There\\'s a lot of good that can be said for this cartoon; the backgrounds are rich, lushly colored and full of nicely done art deco details. The animation is up to the usual studio standards of the time, which are unquestionably higher than those of the present day. However, I find it tedious for a number of reasons.<br /><br />The Music: It\\'s definitely not up to Scott Bradley\\'s usual standards. Although it\\'s probably supposed to be evocative of a \"Great Gatsby\" setting, it ends up being dreary, sleepy, repetitious AND monotonous (repetitious and monotonous are not the same, as Beethoven\\'s 5th Symphony attests). Since most people (including me) tend to close their eyes when they yawn, there\\'s a lot of the visual part of the cartoon that will be missed by the average viewer.<br /><br />The Storyline: I\\'m not giving away any secrets that aren\\'t already in the plot summary - country good, city bad. This is a common theme in films, both animated and live, from this era. It\\'s a misplaced nostalgia for a nonexistent rural idyll, which, in the present day, is reflected in a similar nostalgia for \"values\" that never were.',\n",
            "       b\"I came here for a review last night before deciding which TV movie to settle in front of, and those I found made this one look unmissable. How misled I feel!<br /><br />Firstly, it needs to be pointed out up front that this is very much a housewife's daytime movie. The performances are wooden, every sentence is an attempt at 'poignant' in the way that housewife's daytime movies and bad soap operas always are, and it is based in that predictable and well-trodden premise that men (particularly soldiers) are essentially violent and incompassionate. The whole movie is about the 'drama' apparent in the moments when the male characters threaten to develop a second dimension.<br /><br />If that sounds tolerable (or even enjoyable) to you, then be warned. Linda Hamilton's German accent, while quite good, is painfully distracting - as is her face, for some reason. The other performances are no doubt an enduring source of embarrassment to their perpetrators, with painfully thin and obvious characterizations being the order of the day. There are few surprises, but do watch for the 'Monty Pythonesque' endless supply of food and drink that miraculously appears from the hungry soldiers' knapsacks!<br /><br />I wasn't expecting action, but I had hoped for beautiful or textural or emotionally charged. What I got was a particularly bad Christmas 'feelgood' story that will have an intelligent audience cringing with the crapulence of it all.<br /><br />Watch it under the folowing circumstances: 1: There's nothing else on. 2: You are a fan of predictable 'housewife takes on men and wins' TV movies. 3: The only way you can appreciate a true story is when Hollywood turns it into a feature film. 4: You've imbibed enough nog that your emotions are easily stirred by unsophisticated storytelling.\",\n",
            "       b\"I went in expecting the movie to be completely dumb. With such a low expectation, any form of entertainment would be a pleasant surprise. The soundtrack was the best part of the movie, but poking fun at the nonsense that goes on in singles wards was also amusing.<br /><br />This said, there were many things about The Singles Ward that were completely annoying. The entire film was poorly dubbed and made watching mouths while listening to their voices very irritating. This lack of professionalism was surpassed only by the cameos of Mormon celebrities who have no business acting.<br /><br />This film will do well among Mormondom, especially in college communities where singles ward exist. However the conclusion will offer no hope for the poor losers who find themselves unmarried. (Only the pretty girls in the Singles Ward get married, the fat, ugly ones don't, but all the ugly men do) Ultimately we realize that the whole film was an advertisement for LDSSingles.com\",\n",
            "       b'This movie was on the Romance channel, and I thought it might be a goofy 80\\'s movie that would be enjoyable on some level, so my brother and I watched it. Boy did it suck. Boy gets crush on girl--correction, his *dream*-girl (apparently there is a difference; and I\\'m surprised he realized she was his dream girl--he was smitten with her from over 30 feet away. I guess that just goes to show the power of dream-girls), boy ends up masquerading as a female to be near dream-girl (creative in the sense that it\\'s a far-out plan, but un-creative in the sense that there are probably better solutions one might think up), awkward situations ensue, a match is made (all of which takes seems to take place around late afternoon--either the location was somehow responsible for this odd lighting, or the actors had to wait until they got off of their day-jobs to come to the set; I suspect the latter). Very clumsily done, very pathetic. It\\'s almost never even amusing *accidentally*, so there really is nothing to redeem it. Unless you\\'re interested in seeing Chad Lowe\\'s early days, before he finally got his piece of the pie with his role as the HIV-positive gay guy on the series \"Life Goes On\", or Gail O\\'Grady who was on NYPD Blue and probably got to stare at Dennis Franz\\'s buttocks). But those are unlikely motives--I\\'d say \"systematic derangement of the senses\" would be a more justified purpose. I\\'m surprised I watched it all. I guess it\\'s the kind of thing where, halfway through, you find yourself *still* watching due to some morbid, self-flagellistic inner-issue, and think you might as well finish it so you can tell your friends and family that you actually sat through such a horrible movie, on the off-chance that it\\'ll garner you some sympathy for the questionable state of your mental health. Can *You* Take the Challenge?',\n",
            "       b'I had numerous problems with this film.<br /><br />It contains some basic factual information concerning quantum mechanics, which is fine. Although quantum physics has been around for over 50 years, the film presents this information in a grandiose way that seems to be saying: \"Aren\\'t you just blown away by this!\" Well, not really. These aren\\'t earth shattering revelations anymore. At any rate, I was already familiar with quantum theory, and the fact that particles have to be described by wave equations, etc. is not new.<br /><br />The main problem I have with this movie, however, is the way these people use quantum theory as a way of providing a scientific basis for mysticism and spiritualism. I don\\'t have any serious problem with mysticism and spiritualism, but quantum mechanics doesn\\'t really have anything to do with these things, and it should be kept separate. The people they interviewed for this movie start with the ideas of quantum theory and then make the leap to say that simply by thinking about something you can alter the matter around you, hence we should think positively so as to have a positive impact on the world and make our lives better. The reasoning is completely ridiculous, and the conclusions do not logically follow from quantum theory. For every so called \"expert\" that they interviewed for this film, there are scores of theoretically physicists who would completely disagree. They would point out, quite rightly, that the unpredictability of the subatomic world does not lend support to mystical notions about our spiritual connectedness.<br /><br />It disturbs me that people are going to see this film and completely eat it up because it leaves them with a nice positive feeling. The main thrust of the film is based on a total misinterpretation of quantum theory, and it is as bad in its reasoning as any attempt to justify organized religion with similar pseudo-scientific arguments.<br /><br />Avoid this film.<br /><br />Oh yeah. At one point, one of the \"experts\" says that since throughout history most of the assumptions people have made about the world turned out to be false, therefore the assumptions we currently hold about the world are also likely to be false. Huh? That totally does not follow. And even if it did, I don\\'t see how that helps his argument. I mean, if his ideas ever became common assumptions then I guess we would have to assume that they are false too, based on his own reasoning.',\n",
            "       b\"This movie makes Canadians and Brits out to be asinine, moronic idiots. The men get stoned/drunk, and then they yell/beat each other up in almost every scene. The women are superfluous to the story \\xc2\\x96 I do not understand what they are there for \\xc2\\x96 they spend every scene causing a ruckus, or worse, milling around like mesmerized cattle. Apparently, Canadian women are either quarrelsome vulgar tramps or hulking hippie chicks. It's the standard knocked-up girlfriend, her loser boyfriend and his wicked mother ludicrousness that we have seen in countless movies before.<br /><br />Every character here is a carping, infantile stereotype. Not to mention that they all looked like they need a shower! And the idea of any kind of scene implying sex with George Wendt \\xc2\\x96 shudder \\xc2\\x96 is enough to make anyone gag! I watched the movie because Samuel West was in it \\xc2\\x96 but I cannot understand why he would have accepted a role like this. Maybe he needed the money. Ian Tracey is a superb actor - the only one with a vague redeeming moment, but his talent is wasted here.<br /><br />As for the rest of the plot \\xc2\\x96 the three imbeciles trying to get their dope back \\xc2\\x96 yawn - or Karl \\xc2\\x96 who is dead, but who is actually a character very much alive in the minds of those left behind (almost like Rebecca in Alfred Hitchcock's masterpiece \\xc2\\x96 although I am ashamed to even have thought to compare these two films), why even bother? Karl is so galling that you find the circumstances of his death gratifying.<br /><br />By the end of this wretched movie, I thought they would all have been better off going down with him on that boat!\",\n",
            "       b'Sistas in da hood. Looking for revenge and bling bling. Except da hood is a wild west town in the late 1800s. I do not remember any westerns like this when I was growing up. What would Randolph Scott say? If he saw Lil\\' Kim, he might say, \"Alright! I have to admit that I tuned into this just to see her. Bare midriffs and low cut blouses are not the staple of the usual cowboy flick, but these are the cowgirls, and they are fine.<br /><br />Now, don\\'t go looking for any major story here, and the usual stuff of ghetto crime drama are here in a different setting. And, when\\'s the last time you heard John Wayne call someone, \"Dawg\"? And, I don\\'t remember the Earp brothers hugging and kissing before they marched to the OK Corral.<br /><br />I watch this on BET, so I missed the action that got it an R rating, but I doubt if I will buy the DVD to see it unless I can be assured it was Lil\\' Kim in that action.',\n",
            "       b\"Okay, let's face it. this is a god-awful movie. The plot (such as it is) is horrible, the acting worse. But the movie was made for one reason and one reason only, like all of those awful Mario Lanza movies...just to hear the voice of the star, in this case Pavarotti in his prime. Okay, so maybe the Lanza movies were also an excuse for him to hit on women, but this movie is about hearing Luciano. That alone is worth watching the movie. A big opera star stuck on himself faces his fears, finds humility and love along the way, and belts out a lot of hit numbers, too.<br /><br />I must admit I'm prejudiced on a number of levels. I'm Italian. I'm a big Pavarotti fan (is there anything about Pavarotti that isn't big, including his fan base?). And when I first saw this movie I was going out on my own, seeing the height and depth of Life's possibilities and in love for the first time. So as awful as this movie is, the beautiful voice and memories are enough to make me breathe deep of life and love again.<br /><br />Yes, it's corny and awful. But the voice is immortal and timeless, and the voice is what it's all about. So I give this movie a high rating in hopes that someone who has never heard Pavarotti before will listen and watch and enjoy a new level of music and love, especially since he is now gone. Like Italian food that you've never tried before, try it! You may be pleasantly surprised, as a Luciano lover or prospective Pavarotti peep.\",\n",
            "       b'The Dukes of Hazzard will academy awards!! Best actor and actress 4 the persons who can say with a straight face that this was a great movie.<br /><br />This \"movie\" was a torture to watch. So sad how an weekly half hour entertainment was destroyed by these amateurs.The only good thing about this crap was the car! I remember when Daisy was a real threat to look 4ward 2. Who\\'s the moron that decided that Jessica Simpson is hot?! We know she can\\'t act but come on. In the TV show Daisy was a fox and brunette.<br /><br />All members who contributed in these waste of time please please please don\\'t even think about makin a sequel, a prequel or anything that\\'s got 2 do with a former TV show.<br /><br />I gave a empty DVD so this \"movie\" could be burned 4 me. I sat trough it and i want my money back!',\n",
            "       b'I thought this movie was terrible. I\\'m Chinese, so I thought everything was totally wrong. Many of the facts were incorrect. The only thing right about Chinese history in the movie was when Wendy\\'s mother explained to her husband about the statues that guarded ShiHuangDi. I also thought the fight scenes were very cheesy and fake. Many of the actors and actresses were not very great. Some of the jokes that were supposedly \"funny\" were really stupid. I think this movie should receive the worst possible rating it could get. Disney has really got to get more information about Chinese history if they want to create an extravagant movie. Mulan was quite accurate. Watch this movie if you want to waste some time.'],\n",
            "      dtype=object)>, <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
            "array([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,\n",
            "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0], dtype=int32)>)\n"
          ]
        }
      ],
      "source": [
        "for example in raw_train_ds.take(1):  # Adjust the number as needed\n",
        "    print(\"Example:\", example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2ahESIWIDiY",
        "outputId": "b1d2c0b8-e976-4bed-f60b-729a0f753254"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktFvHm-BJgK6",
        "outputId": "cb58bcea-3154-46ab-aa48-a5d189b211ef"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "625"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkGnkOnrKSHC",
        "outputId": "e6bdb867-4616-4b1b-885a-b8234ffcb038"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example: (<tf.Tensor: shape=(32,), dtype=string, numpy=\n",
            "array([b'Belmondo is a tough cop. He goes after a big-time drug dealer (played by Henry Silva, normally a great villain - see \"Sharky\\'s Machine\"; but here he is clearly dubbed, and because of that he lacks his usual charisma). He goes to the scuzziest places of Paris and Marseilles, asks for some names, beats up some people, gets the names, goes to more scuzzy places, asks for more names, beats up more people, etc. The whole movie is punch after punch after punch. It seems that the people who made it had no other ambition than to create the French equivalent of \"Dirty Harry\". Belmondo, who was 50 here, does perform some good stunts at the beginning; apart from those, \"Le Marginal\" is a violent, episodic, trite, shallow and forgettable cop movie. (*1/2)',\n",
            "       b'Wow. The only people reviewing this positively are the Carpenter apologists. I know a lot of those. The guys that\\'ll watch John Carpenter squat on celluloid and pinch out a movie and proclaim it a masterwork of horror. This \"movie\" is utter crap. It looks and sounds like a porno (good lord, the soundtrack is awful...), and has sub-par porn acting, which is shocking, because normally Ron Perlman is really a very good actor. I honestly have no idea what Carpenter was thinking when making this. Most likely \"Beans, beans, beans..\" until somebody fed him and rolled him up into a blanket for the day... They say nothing about the abortion debate whatsoever, when they could have had a very interesting central theme (how do religious zealot anti-abortionists feel when it\\'s the devil\\'s baby?) but instead they chose to have Ron Perlman and his terribly acted kids kill a bunch of people and have the horribly cast doctors try to calm the hysterically bad pregnant girl. Not a single person from this episode or what have you should come away unscathed. It\\'s just awful. Like, Plan 9 From Outerspace awful. Like, good god please would somebody turn it off before I soil myself awful. Try watching this and The Thing in the same day and your mind will implode.',\n",
            "       b'Even though it has one of the standard \"Revenge Price Plots,\" this film is my favorite of Vincent Price\\'s work. Gallico has that quality that is missing in so many horror film characters- likeability. When you watch it, you feel for him, you feel his frustration, the injustices against him, and you cheer him on when he goes for vengeance, even though he frightens you a little with his original fury. As the film goes on, his character becomes tragic. He\\'s committed his murder, but now he must kill to cover that up. And again to cover that one up. And again... your stomach sinks with his soul as it goes down its spiral- like watching a beloved brother turn into a hood. Even if the revenge story is of old, the plot devices themselves are original- Gallico uses his tricks to kill in more and more inventive ways. A shame this one isn\\'t available for home veiwing.',\n",
            "       b\"This is another one of those movies that could have been great. The basic premise is good - immortal cat people who kill to live, etc. - sort of a variation on the vampire concept.<br /><br />The thing that makes it all fall apart is the total recklessness of the main characters. Even sociopaths know that you need to keep a low profile if you want to survive - look how long it took to catch the Unibomber, and that was because a family member figured it out.<br /><br />By contrast, the kid (and to a lesser extent, the mom) behave as though they're untouchable. The kid kills without a thought for not leaving evidence or a trail or a living witness. How these people managed to stay alive and undiscovered for a month is unbelievable, let alone decades or centuries.<br /><br />It's really a shame - this could have been so much more if it had been written plausibly, i.e., giving the main characters the level of common sense they would have needed to get by for so long.<br /><br />Other than that, not a bad showing. I loved the bit at the end where every cat in town converges on the house - every time I put out food on the porch and see our cats suddenly rush in from wherever they were before, I think of that scene.\",\n",
            "       b'\"Emma\" was a product of what might be called by the First Great Jane Austen Cycle of the mid-nineties, and it was recently shown on British television, doubtless because of the interest in the author created by the Second Great Jane Austen Cycle which started with \"Pride and Prejudice\" two years ago. We currently have in the cinemas the Austen biopic \"Becoming Jane\", and ITV have recently produced three TV movies based on Austen novels. These include \"Northanger Abbey\", the only one of the six major novels not to have been filmed previously, so the cycle should now be complete. No doubt, however, there will be more to come in the near future. (There is, after all, her juvenile \"Love and Freindship\" (sic), the short novella \"Lady Susan\", and someone, somewhere, has doubtless supplied endings to her two unfinished fragments \"The Watsons\" and \"Sanditon\". Then there are all those Austen sequels churned out by modern writers\\xc2\\x85\\xc2\\x85\\xc2\\x85).<br /><br />The main character is Emma Woodhouse, a young lady from an aristocratic family in Regency England. (Not, as some reviewers have assumed, Victorian England- Austen died before Queen Victoria was even born). Emma is, financially, considerably better off than most Austen heroines such as Elizabeth Bennett or Fanny Price, and has no need to find herself a wealthy husband. Instead, her main preoccupation seems to be finding husbands for her friends. She persuades her friend Harriet to turn down a proposal of marriage from a young farmer, Robert Martin, believing that Harriet should be setting her sights on the ambitious clergyman Mr Elton. This scheme goes disastrously wrong, however, as Elton has no interest in Harriet, but has fallen in love with Emma herself. The speed with which Emma rejects his proposal makes one wonder just why she was so keen to match her friend with a man she regards (with good reason) as an unsuitable marriage partner for herself. This being a Jane Austen plot, Emma turns out to be less of a committed spinster than she seems, and she too finds herself falling in love, leading to further complications.<br /><br />Emma always insists that she will not marry without affection, and when she does find a partner, the handsome Mr Knightley, we feel that this will indeed be an affectionate marriage. It does not, however, seem likely to be a very passionate one (unlike, say, that of Elizabeth Bennett and Mr Darcy). Knightley, who is sixteen years older than Emma (she is 21, he 37), and related to her by marriage, is more like a father-figure than a lover. Much more of a father-figure, in fact, than her actual father, a querulous and selfish old hypochondriac who seems more like her grandfather. When Emma is rude to her unbearably garrulous and tedious friend Miss Bates, it is Knightley who chides her for her lack of manners. (His surname is probably meant to indicate his gentlemanly nature- nineteenth-century gentlemen liked to think of themselves as the modern equivalent of mediaeval knights with their elaborate codes of chivalry). Both Gwyneth Paltrow and Jeremy Northam play their parts very well, but this is not really one of the great screen romances.<br /><br />Of the other characters, I liked Juliet Stephenson\\'s vulgar Mrs Elton and Toni Collette\\'s Harriet. I know that in the novel Harriet was a na\\xc3\\xafve young teenager, whereas here she is more like the character Collette played in \"Muriel\\'s Wedding\"- a gauche, slightly overweight twentysomething, fretting about her chances of finding a man. Nevertheless, I felt that this characterisation worked well in the context of the film and did not detract from Austen\\'s themes.<br /><br />\"Emma\" is one of Austen\\'s more light-hearted works, without the darker overtones of \"Mansfield Park\" or even \"Pride and Prejudice\", and this is reflected on screen. We see a world of beauty and grace, full of stately homes and elegant costumes and fine manners. Apart from the ruffianly gypsies, who make a very brief appearance, the only \"poor\" people we see are Mrs Bates and her daughter, and, as they live in the sort of picturesque rose-strewn thatched cottage which today would change hands for over \\xc2\\xa3500,000, we can be sure that their poverty is relative, not absolute. In Emma\\'s world, poverty is defined as not having your own stately home. This is, of course, not a comprehensive picture of early nineteenth-century life, but nobody has ever claimed Austen as the Regency equivalent of a kitchen-sink realist. Sophisticated romantic comedy, combined with a keen eye for analysing human character, was more in her line.<br /><br />I would not rate this film quite as highly as the 1994 \"Sense and Sensibility\" or the recent \"Pride and Prejudice\"- it tends to drag a bit in the middle, although it has a strong beginning and strong ending- but it is, in the main, a highly enjoyable Austen adaptation. 7/10',\n",
            "       b\"I am shocked. Shocked and dismayed that the 428 of you IMDB users who voted before me have not given this film a rating of higher than 7. 7?!?? - that's a C!. If I could give FOBH a 20, I'd gladly do it. This film ranks high atop the pantheon of modern comedy, alongside Half Baked and Mallrats, as one of the most hilarious films of all time. If you know _anything_ about rap music - YOU MUST SEE THIS!! If you know nothing about rap music - learn something!, and then see this! Comparisons to 'Spinal Tap' fail to appreciate the inspired genius of this unique film. If you liked Bob Roberts, you'll love this. Watch it and vote it a 10!\",\n",
            "       b\"Children love dinosaurs. It's somewhat part of their culture. But they've got The Land Before Time. The original. At least that movie had heart. This. This movie is just plain pathetic. Just because kids love dinosaurs doesn't mean you can just slap together any old story and show it to the children. This movie has no plot, the whole premise is stupid, and it's more by the numbers stuff. Not as soul sucking as Theodore Rex, but it's lightyears away from being a Land Before Time.\",\n",
            "       b\"The 60\\xc2\\xb4s is a well balanced mini series between historical facts and a good plot. In four deliveries, we follow a north American family, with 3 members. But we don't only see them. We also follow the story of several characters as a black reverend, an extremist student leader, and a soldier in Vietnam. The filmography is just extraordinary. In the first chapters, we see some shots of the Vietnam war, in between the scenes. The next chapter, doesn't start where the last one finished, it starts some time after, giving us a little mystery on what happened. In general, The 60\\xc2\\xb4s mini series, is a must see, not only for hippies fanatics, but for everyone with little curiosity about the topic.\",\n",
            "       b'I have no idea what idiots gave this movie a Palm D\\'Or at the 1999 Cannes Film Festival because it was atrocious! I actually watched the entire thing simply because I couldn\\'t believe that someone would make such a worthless film. There is nothing interesting about the plot, the characters are devoid of depth and there is no attempt at giving any sort of ambiance with music or sound effects. Also, if you do decide to waste 2 hours of your life by watching this film, be sure to bring something to throw up in because the cinematography is simply someone running around with a hand-held camcorder and half the time you can\\'t even see the main subjects. This style has been used much more successfully in movies such as \"Blair Witch\" because it creates suspense. In Rosetta, there is no plot and no suspense to which that style would lend anything. I should have known better when it came on at 2 o\\'clock in the morning that it was going to be horrible.',\n",
            "       b'This film is not even worth walking to the movie theatre. No jokes, but stupid and boring laughing on repeated disgusting stuff. The music and the girls are great, unfortunately you have to watch the whole movie to enjoy them. It was weak, very very weak.',\n",
            "       b'I am a huge fan of Simon Pegg and have watched plenty of his movies until now and none of them have ceased to make me laugh. Neither did How to lose friends and Alienate People.<br /><br />This movie is essentially about a man good as pissing people off. However, he has an innate set of ethics that prevents him from doing things that might just make him famous. But in the end he ends up doing them, the culture of life.<br /><br />The movie is well toned with humor, romance, good acting and also a bit of a lap dance. Its one of those movies where you could just be happy when it ends.',\n",
            "       b\"I agree with the previous comment, what a disappointment. Rented it thinking it was going to be a good movie since Mira and Olivier where in it. I was surprised by their performance, expected more since they're good actors.<br /><br />Thought it was a slow beginning but it got worse. I even laughed at some bad stunts!! when is supposed to be a mystery movie. You can even guess who is the killer beforehand!!! <br /><br />For real what happened?? <br /><br />Sorry to say but don't even bother you'll waste time and money.<br /><br />Boring!!!\",\n",
            "       b\"Flat, ordinary thriller about a conniving woman who deceives all those she supposedly loves in order to boost her bank account. Nicole Kidman plays the deceptive Tracey, married to the doting Andy (Bill Pullman). When an old school friend of Andy's named Jed Hill (Alec Baldwin) turns up as the resident surgeon, trouble is not far behind him.<br /><br />Script fails in that it does not carefully develop the promising premise into an effective, tantalising thriller, and the severe lack of character motivation, background and development leaves the whole show reaching. None of the cast are able to generate interest in their shallow characters, especially Bill Pullman, whose own inexplicably curious Andy is impossible to believe.<br /><br />Poor director Harold Becker is left trying to resurrect an impossibly dead project, and is unable to make entertainment from any of it. By the time the 'secret' of the plot is revealed, you just won't care.<br /><br />At least the cinematography has Massachusetts looking good. Also stars George C. Scott, Peter Gallagher and Josef Sommer.<br /><br />Sunday, February 25, 1996 - T.V.\",\n",
            "       b\"I pity people calling kamal hassan 'ulaganaayakan' maybe for them ulagam is tollywood ! comeon guys..this movie is a thriller without thrill..<br /><br />come out of your ulagam and just watch some high class thrillers like The Usual Suspects or even The Silence of the Lambs.<br /><br />technically good but style over substance kamal doesn't look like a police officer, there is no thrill whatsoever dragging and boring till end you might be saving 3 valuable hrs of your life if u skip watching this movie.<br /><br />kamal at his best is the best in tollywood\",\n",
            "       b'I saw the description of the movie on TCM and only let it run because I like both Peter Ustinov and Maggie Smith, so I was delightfully surprised to find that I really liked the movie and found it quite exceptional. Of course, it is seriously dated, but as a period piece it is well worth watching just for the subtle humour in insight into life and lifestyle almost forty years ago. Now the only problem is trying to find it on DVD so I can watch it more often. I also was quite taken with the performances of Smith and Ustinov as the leads, and of Karl Malden, Bob Newhart, and the cameo appearances by Robert Morley and Cesar Romero.',\n",
            "       b'Ah, noir. My favourite genre. Otto Preminger\\'s follow-up to \"Laura\" is a film noir set in a postwar New York, where corruption and violence run rampant. It stars Dana Andrews as Sergeant Mark Dixon, a detective whose brutal tactics have landed him in hot water with his superiors. <br /><br />When he accidentally kills a murder suspect, Dixon tries to pin the blame on crime boss Tommy Scalese. Dixon is close to achieving his goal when he becomes involved with the dead man\\'s wife, the beautiful Morgan Taylor. Of course, in typical noir fashion, things quickly go down hill.<br /><br />While the film does nothing interesting camera or narrative wise, it does have a constant tone of dread and gloom. Like most great noirs, it is also wonderfully paced, sucking the viewer in right from the start.<br /><br />Still, like most of Preminger\\'s workmanlike films, it\\'s not something I\\'d watch again. It lacks the verbal wit of Wilder, the visual flamboyance of Hitchcock or the spatial experimentation of Welles. Like Lang\\'s later work, \"Sidewalk\" feels very much a \"clone\" of what a noir should be, instead of something really artistically genuine. <br /><br />Thematically the film is nothing special. It\\'s about a cop who finds himself slowly becoming a criminal. In one scene Dixon explains that his father himself was a small time crook, the film flirting with the notion of predestination, but this one scene is as far as Otto goes, or dares to take his ideas.<br /><br />The camera work is likewise disappointing. There\\'s no intelligence in Otto\\'s camera. No effort is made to assign the camera to anything. It doesn\\'t play with space or architecture or empahsise the step-by-step police procedural. It\\'s just tripod set ups with the occasional dolly in and out. Meaningless, though most people don\\'t care about these things. <br /><br />There is, however, one good shot where our hero is trapped in a car full of gangsters (noir cage) which itself enters a vehicle-lift (another cage) and is taken up into the chief gangster\\'s lair. Like Lynch\\'s \"Blue Velvet\", Otto retopologizes the film, constantly likens \"upstairs\" to hell. This works well, but the set design fails to reinforce it. <br /><br />7/10 - Otto seems content to follow the Hawksian mould of what a noir is, rather than toy with the possibilities of where noir can go. Like \"Detour\", \"Night in the City\", \"Scarlet Street\", \"In A Lonely Place\", \"Act of Violence\", \"Bomberang\" and \"Johnny Eager\", \"Where the Sidewalk Ends\" is one of those well made second tier film noirs. It\\'s competent and engaging, but lacks that extra special magic.',\n",
            "       b'I saw this movie way back at the first theatrical release, in a justifiably empty theater. Believe it or not, after decades of watching movies, this one still sticks clearly in my mind as the worst movie of all time; or at least the worst that I would allow myself to watch.<br /><br />The acting is far beneath the standard set by any random group of drunken high-school students yanked off the street and forced to learn their lines in 5 minutes or less.<br /><br />After the first shock of disbelief, we laughed for a while as each scene hit new lows. But after a while, even that dubious pleasure wore off and it just got to be really sad.',\n",
            "       b\"Okay, let's face it. this is a god-awful movie. The plot (such as it is) is horrible, the acting worse. But the movie was made for one reason and one reason only, like all of those awful Mario Lanza movies...just to hear the voice of the star, in this case Pavarotti in his prime. Okay, so maybe the Lanza movies were also an excuse for him to hit on women, but this movie is about hearing Luciano. That alone is worth watching the movie. A big opera star stuck on himself faces his fears, finds humility and love along the way, and belts out a lot of hit numbers, too.<br /><br />I must admit I'm prejudiced on a number of levels. I'm Italian. I'm a big Pavarotti fan (is there anything about Pavarotti that isn't big, including his fan base?). And when I first saw this movie I was going out on my own, seeing the height and depth of Life's possibilities and in love for the first time. So as awful as this movie is, the beautiful voice and memories are enough to make me breathe deep of life and love again.<br /><br />Yes, it's corny and awful. But the voice is immortal and timeless, and the voice is what it's all about. So I give this movie a high rating in hopes that someone who has never heard Pavarotti before will listen and watch and enjoy a new level of music and love, especially since he is now gone. Like Italian food that you've never tried before, try it! You may be pleasantly surprised, as a Luciano lover or prospective Pavarotti peep.\",\n",
            "       b\"This is widely viewed in Australia as one of the best cop dramas ever produced here ... and for my money, anywhere. It's raw, gritty, the characters are real, the situations are believable and it doesn't shy away from the darker side of life confronted every day by cops and the criminals, victims, lawyers and other people in their various orbits.<br /><br />This show ran for 2 seasons and was discontinued because the show didn't sell well overseas. We are all sorry for its loss: however, like Fawlty Towers, we will be able to revere this as a limited-length series of uniformly high quality.\",\n",
            "       b'What a lovely heart warming television movie. The story tells of a little five year old girl who has lost her daddy and finds it impossible to cope. Her mother is also very distressed ..only a miracle can alleviate their unhappiness.Which all viewers hope will materialise. Samantha Mathis is brilliant as the little girl\\'s mum ,as she was as the nanny in\" Jack and Sarah\",worth watching if you like both Samantha Mathis and happy; year tear jerking movies! Ellen Burstyn is, as, always a delightful grandmother in this tender and magnificently acted movie. Jodelle Ferland (the little five year old) is charming and a most convincing young actress. The film is based on a true story which makes it so touching.\"Mermaid\" is a tribute to the milk of human kindness which is clearly illustrated and clearly is still all around us in this difficult world we live in. \"Mermaid\" gives us all hope ,by realising that there a lot of lovely people in the world with lot\\'s of love to give. James Robson Glasgow Scotland U.K.',\n",
            "       b\"This is comedy as it once was and comparing this with the two remakes, THE MONEY PIT and ARE WE DONE YET?, only points out all the more how the 40's movie makers had a flair for comedy which has since, regretfully, been lost.<br /><br />I was 15 when I first saw this and even at that tender age, there was much I could laugh at. Now of course being familiar with adult frustrations, I see a whole lot that I missed as a youth.<br /><br />The three main actors...Cary Grant, Myrna Loy, and Melvyn Douglas...interacted perfectly, but the core of the movie lies in the frustrations encountered in achieving a dream. It's never as easy and free of unseen complications as one envisions.<br /><br />All in all, this is a classic comedy which still stands above the attempts to remake it.\",\n",
            "       b'I thought Rachel York was fantastic as \"Lucy.\" I have seen her in \"Kiss Me, Kate\" and \"Victor/Victoria,\" as well, and in each of these performances she has developed very different, and very real, characterizations. She is a chameleon who can play (and sing) anything!<br /><br />I am very surprised at how many negative reviews appear here regarding Rachel\\'s performance in \"Lucy.\" Even some bonafide TV and entertainment critics seem to have missed the point of her portrayal. So many people have focused on the fact that Rachel doesn\\'t really look like Lucy. My response to that is, \"So what?\" I wasn\\'t looking for a superficial impersonation of Lucy. I wanted to know more about the real woman behind the clown. And Rachel certainly gave us that, in great depth. I also didn\\'t want to see someone simply \"doing\" classic Lucy routines. Therefore I was very pleased with the decision by the producers and director to have Rachel portray Lucy in rehearsal for the most memorable of these skits - Vitameatavegamin and The Candy Factory. (It seems that some of the reviewers didn\\'t realize that these two scenes were meant to be rehearsal sequences and not the actual skits). This approach, I thought, gave an innovative twist to sketches that so many of us know by heart. I also thought Rachel was terrifically fresh and funny in these scenes. And she absolutely nailed the routines that were recreated - the Professor and the Grape Stomping, in particular. There was one moment in the Grape scene where the corner of Rachel\\'s mouth had the exact little upturn that I remember Lucy having. I couldn\\'t believe she was able to capture that - and so naturally.<br /><br />I wonder if many of the folks who criticized the performance were expecting to see the Lucille Ball of \"I Love Lucy\" throughout the entire movie. After all, those of us who came to know her only through TV would not have any idea what Lucy was really like in her early movie years. I think Rachel showed a natural progression in the character that was brilliant. She planted all the right seeds for us to see the clown just waiting to emerge, given the right set of circumstances. Lucy didn\\'t fit the mold of the old studio system. In her frustrated attempts to become the stereotypical movie star of that era, she kept repressing what would prove to be her ultimate gifts.<br /><br />I believe that Rachel deftly captured the comedy, drama, wit, sadness, anger, passion, love, ambition, loyalty, sexiness, self absorption, childishness, and stoicism all rolled into one complex American icon. And she did it with an authenticity and freshness that was totally endearing. \"Lucy\" was a star turn for Rachel York. I hope it brings a flood of great roles her way in the future. I also hope it brings her an Emmy.',\n",
            "       b'<br /><br />Ok, well I rented this movie while I was bed ridden hopped up on pain killers, and let me say, It didn\\'t help the film any.<br /><br />The film is about a man who buys a car as he is going through a midlife crisis, he loves the car more than anything around him, one day his wife decides to borrow the car. Since I don\\'t want to spoil (not that there was anything to spoil) I shall let your imagination figure out the \"Zany\" (and I use that word lightly) antics that follow.<br /><br />I had to fight to stay awake through this snore a minute sleeper of a film, and I would like to say that if you are venturing to the movie store and are thinking about being adventurous, please don\\'t, it\\'s a waste of the film it was printed on.<br /><br />Then again I could be wrong...',\n",
            "       b\"For me, this movie just seemed to fall on its face. The main problem for me was the casting of Glover as a serial killer. I don't know whether this grows out of type-casting or simply his demeanor, but I doubt Glover could ever portray a convincing villain. He's a good guy, and that's always obvious in his performances. Other than that the film is your run of the mill serial killer story. Nothing very innovative .\",\n",
            "       b'Literally every aspect of this science-fiction low-budget flick falls under the categories that have been classified for its predecessors, contemporaries, and those to follow. Bad special effects, a weak storyline, ridiculous amounts of blood and gore, annoying and pointless characters, all that you can expect. \"Attack of the Sabretooth\" is about a new vacation resort where the proprietors are genetically engineering Smilodon cats for an attraction. The cats escape and begin to kill people, the guy running the show wants to save them and not warn the unsuspecting visitors about them, and there is a band of visitors and some employees who rebel and plan to kill the cats.<br /><br />Special effects-wise, the film is about an average achievement given its budget. The sabretooths are portrayed through poor CGI. Amazingly, though, the cats look more realistic in an up-close, detailed shot rather than the longer, more distant shots where the CGI is better concealed. Their attacks are recklessly bloody and distasteful. Just as you\\'d expect, they attack, rip off some arms and legs, and leave very little behind. This is part of the reason why the film descends into poor schlock.<br /><br />The plot and characters are just as horrendous. We have some college kids who come to the island and they plan a scavenger hunt. And take it very, VERY seriously. Even so much as to trespass on private property, tamper with security systems, and steal. Why are they taking a simple game so seriously? Did I miss something? Was there money involved? Or were they sent to do it? I don\\'t know, I could barely follow the film. But it seemed to me like they were just doing it for the fun of doing it. Even so, they went too far for normal.<br /><br />\"Attack of the Sabretooth\" is a very poor film. Even for a low-budget sci-fi flick, it is a very poor and cheap example. It will bore most viewers to tears, might be attractive for some, and will make you chuckle and laugh all the way through. And keep in mind, this is not a comedy, this is a cheap horror flick, so it\\'s not suppose to be comical.',\n",
            "       b\"This seems like two films: one a dreary, pretentious lengthy saga about an ac-tor who is taken over by the parts he plays; the other a brilliant social comment about a middle aged divorce who is picked up by a waitress. Shelley Winters is wonderful as a waitress with another business on the side. She drops heavy hints about the need for connections, her certificate in massage and her desire to get into the modelling game. I love the glimpse of her seedy flat with a kitchenette behind a curtain, and her terrible seducing outfit of navel-revealing, puff-sleeved crochet top.<br /><br />Do actors get Oscars for Shakespeare? We know they Oscars for impersonating disabled people, wearing a lot of prosthetics, or pretending to be mad. The Shakespearean scenes (which go ON and ON) are embarrassing and dated. And so are the 'going mad' scenes where Tony looks distracted while listening to his own voice-over.<br /><br />By the way, Anthony John is not aristocratic. He makes it quite clear in an early scene that he used to be a chorus boy. When he quotes his father's advice, he slips into a Cockney accent.\",\n",
            "       b'What an entertaining movie. Astaire and Randolph Scott are in the Navy. Astaire has to woo back Ginger in San Francisco, while Scott must be persuaded that Harriet Hilliard is worthy of being his wife. Everything works out fine.<br /><br />It\\'s sometimes argued, and I can see why, that Scott\\'s romance with Hilliard is unnecessary and slows the plot down. Especially painful are Hilliard\\'s singing of sappy love songs. I could have done without her singing, right enough, but I found the romance rather touching. Hilliard enters the movie as a music teacher, wearing a pair of spectacles, no makeup, and the ugliest clothes known to man or beast. No wonder Scott avoids her until she grooms herself sexily under the tutelage of her sister Ginger and a slinky Lucille Ball. The problem is that AFTER her makeover she looks like the same irretrievably plain woman as before, only with a glossier outfit. It\\'s tough enough for a homely guy. But at least a man can become wealthy and powerful and popular, then he can collect women anyway, even if he looks like JoJo the Dog-Faced Boy. But what does a plain-looking woman do? The same avenues to romance aren\\'t open to her. Henry Kissinger had groupies. Did Margaret Thatcher? It must be terrible to be an ordinary woman in a culture as cruel as ours. As a kind of a footnote, I must mention that Randolph Scott was rumored to be bisexual by Hollywood gossips who needed some nonsense to chew on, and it\\'s kind of amusing that he has the following line in this film when he\\'s putting homely Hilliard off -- \"Women don\\'t interest me, sister.\" The movie probably gives more quality time to Ginger Rogers than any of her other films with Astaire. And she\\'s marvelous. She\\'s beautiful, sexy, a talented actress and dancer, and the script gives her good comic lines too. \"Let\\'s kiss and make up,\" suggests Astaire. \"Let\\'s just make up,\" she says, \"that will give you something to work on.\" It\\'s also the only film she made with Astaire that gives her a solo number during an audition. (Choreographed by Hermes Pan.) Her performance is accidentally sabotaged by Astaire who slips some Alka Seltzer or something into her water, and she hiccups through her number, burping being too indelicate for the time.<br /><br />As a dancer, I\\'d make a terrific circus elephant so my opinion must be taken as that of an amateur, but I think their dances together are the equal of any they put on film. Their first, during a dancing contest, \"Let Yourself Go,\" is the most wildly exuberant of any I can remember. And the only dramatic duet, \"Let\\'s Face the Music and Dance,\" must be among their finest. The last step as they exit is startling.<br /><br />I like the film for reasons with personal resonance. I remember seeing it for the first time in a theater in downtown San Diego, next to a shop called \"The Seven Seas\" that catered to sailors. I was in uniform at the time and was impressed with the treatment of the naval careers of Astaire and Scott. What I mean is, here we have this frothy musical comedy which owed absolutely nothing to historical reality, and yet, unlike most of their other teamings, pays its dues anyway. Jumping ship, Astaire calls out for a \"water taxi.\" I had just taken a water taxi ashore myself. The uniforms are correct to the period (unlike, say, those in \"On the Town\"). And they\\'re worn properly, hats down to two finger-widths above the eyebrow and not cocked back. And there are two incidents in which Astaire runs into a problem with naval authorities and they\\'re both handled with perfect seriousness and are thoroughly believable. An officer stops Astaire from leading a jazz band during inspection and reports to the captain, \"They were playing when the call was sounded, sir. I\\'m certain no breach of discipline was intended.\" It\\'s what an officer -- a good officer -- would say, and nobody laughs.<br /><br />The series was getting a little repetitious by the time \"Follow the Fleet\" was made so instead of putting Fred into high society and a tuxedo, they turned him into a gum-chewing swabbie instead. It was a good idea. And the dance numbers are up to the concept.',\n",
            "       b'Best Stephen King film alongside IT, though this one is more fun than scary. <br /><br />This one\\'s got it all: <br /><br />-a great cast with a Alice Krige and Brian Krause and a fun cameo from King himself;<br /><br />-well dosed horror in an amusing storyline;<br /><br />-great use of music, Santo & Johnny\\'s \"Sleepwalk\" in particular;<br /><br />-likeable characters in a typical King setting: middle of nowhere village;<br /><br />-lots of humor. You can\\'t really get good scares here because it\\'s too much fun and over the top;<br /><br />-old but really nice makeup effects like they don\\'t make anymore!<br /><br />A 4,5 rating: I don\\'t get it really. When was the last time a horror film was as much fun as this one?',\n",
            "       b'There are very few movies that are so funny as this one. I was lucky enough to watch this movie at a theater \"reserved\" for movie buffs like me, so it was not so embarrassing sitting there laughing till my jaw was completely sore and my shirt sleeves were all wet from drying my eyes...<br /><br />At times the story was a bit \"slow\", but that is perhaps for the best - a bit of rest in-between the rolling amongst the aisles (I nearly fell out of the seat...) was most welcome.',\n",
            "       b'The beginning of this movie is excellent with tremendous sound and some nice humor, but once the film changes into animation it quickly loses its appeal.<br /><br />One of the reasons that was so, at least for me, was that the colors in much of the animation are too muted, with too little contrast. It doesn\\'t look good, at least on VHS. Once in a while it breaks out and looks great, but not often Also, the characters come and go too quickly. For example, I would have liked to have seen more of \"Moby Dick.\" When the film starts to drag, however, it picks up again with the entrance of the dragon and then the film finishes strong. <br /><br />Overall, just not memorable enough or able to compete with the great animated films of the last dozen years.',\n",
            "       b\"The 1960's were a time of change and awakening for most people. Social upheaval and unrest were commonplace as people spoke-out about their views. Racial tensions, politics, the Vietnam War, sexual promiscuity, and drug use were all part of the daily fabric, and the daily news. This film attempted to encapsulate these historical aspects into an entertaining movie, and largely succeeded.<br /><br />In this film, two families are followed: one white, one black. During the first half of the film, the story follows each family on a equal basis through social and family struggles. Unfortunately, the second half of the movie is nearly dedicated to the white family. Admittedly, there are more characters in this family, and the story lines are intermingled, but equal consideration is not given to the racial aspects of this century.<br /><br />On the whole, the acting is well done and historical footage is mixed with color and black and white original footage to give a documentary feel to the movie. The movie is a work of fiction, but clips of well-known historical figures are used to set the time-line.<br /><br />I enjoyed the movie but the situations were predictable and the storyline was one-sided.\",\n",
            "       b\"Billy Crystal normally brings the crowd to laughter, but in this movie he and all the rest of them cannot bring any smile on my face.... or perhaps just one. They call it comedy, I say it's a waste of my time.\"],\n",
            "      dtype=object)>, <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
            "array([0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n",
            "       0, 0, 0, 0, 1, 1, 1, 0, 0, 0], dtype=int32)>)\n"
          ]
        }
      ],
      "source": [
        "for example in train_ds.take(1):  # Adjust the number as needed\n",
        "    print(\"Example:\", example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGm10A5HRGXp"
      },
      "source": [
        "Let's take a look at a few reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuxDkcvVIoev",
        "outputId": "2626a604-60b6-41d4-f08a-78191f8ed04e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Review: b'Recipe for one of the worst movies of all time: a she-male villain who looks like it escaped from the WWF, has terrible aim with a gun that has inconsistent effects (the first guy she shoots catches on fire but when she shoots anyone else they just disappear) and takes time out to pet a deer. Then you got the unlikable characters, 30 year old college students, a lame attempt at a surprise ending and lots, lots more. Avoid at all costs.'\n",
            "Label : 0 (neg)\n",
            "Review: b\"Icy and lethal ace hit-man Tony Arzenta (a divinely smooth and commanding performance by Alain Delon) wants to quit the assassination business, but the dangerous mobsters he works for won't let him. After his wife and child are killed, Arzenta declares open season on everyone responsible for their deaths. Director Duccio Tessari relates the absorbing story at a constant snappy pace, maintains a properly serious and no-nonsense tone throughout, stages the stirring shoot-outs and exciting car chases with considerable rip-snorting brio, and punctuates the narrative with jolting outbursts of explosive bloody violence. Delon's suave and charismatic presence adds extra class to the already engrossing proceedings. This film further benefits from sterling acting by a bang-up cast, with praiseworthy contributions by Richard Conte as wise Mafia kingpin Nick Gusto, Carla Gravini as Arzenta's supportive lady friend Sandra, Marc Porel as Arzenta's loyal pal Domenico Maggio, Anton Diffring as ruthless, calculating capo Grunwald, and Lino Troisi as the venomous gangster Rocco Cutitta. Silvano Ippoliti's glossy cinematography boasts several graceful pans. Gianni Ferrio's funky score hits the get-down groovy spot. Erika Blanc and Rosalba Neri pop up briefly in nifty bit parts. Better still, there's no filler to speak of and we even get a decent dab of tasty gratuitous female nudity. The startling conclusion packs a mean and lingering wallop right to the gut. A solid and satisfying winner.\"\n",
            "Label : 1 (pos)\n",
            "Review: b\"An OUR GANG Comedy Short.<br /><br />The Gang coerces Spanky into watching their younger siblings. Caring for these FORGOTTEN BABIES turns out to be quite a chore, leaving the little nipper with no choice but to come up with some ingenious solutions to the baby-sitting problem...<br /><br />Spanky is in his glory in this hilarious little film, arguably his best. Highlight: Spanky's retelling the plot of the TARZAN movie he's recently seen to the audience of infants. Movie mavens will recognize Billy Gilbert's voice in the radio drama.\"\n",
            "Label : 1 (pos)\n"
          ]
        }
      ],
      "source": [
        "for text_batch, label_batch in train_ds.take(1):\n",
        "  for i in range(3):\n",
        "    print(f'Review: {text_batch.numpy()[i]}')\n",
        "    label = label_batch.numpy()[i]\n",
        "    print(f'Label : {label} ({class_names[label]})')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dX8FtlpGJRE6"
      },
      "source": [
        "## Loading models from TensorFlow Hub\n",
        "\n",
        "Here we can choose which BERT model you will load from TensorFlow Hub and fine-tune. There are multiple BERT models available.\n",
        "\n",
        "  - [BERT-Base](https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3), [Uncased](https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3) and [seven more models](https://tfhub.dev/google/collections/bert/1) with trained weights released by the original BERT authors.\n",
        "  - [Small BERTs](https://tfhub.dev/google/collections/bert/1) have the same general architecture but fewer and/or smaller Transformer blocks, which lets you explore tradeoffs between speed, size and quality.\n",
        "  - [ALBERT](https://tfhub.dev/google/collections/albert/1): four different sizes of \"A Lite BERT\" that reduces model size (but not computation time) by sharing parameters between layers.\n",
        "  - [BERT Experts](https://tfhub.dev/google/collections/experts/bert/1): eight models that all have the BERT-base architecture but offer a choice between different pre-training domains, to align more closely with the target task.\n",
        "  - [Electra](https://tfhub.dev/google/collections/electra/1) has the same architecture as BERT (in three different sizes), but gets pre-trained as a discriminator in a set-up that resembles a Generative Adversarial Network (GAN).\n",
        "  - BERT with Talking-Heads Attention and Gated GELU [[base](https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1), [large](https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_large/1)] has two improvements to the core of the Transformer architecture.\n",
        "\n",
        "The model documentation on TensorFlow Hub has more details and references to the\n",
        "research literature. Follow the links above, or click on the [`tfhub.dev`](http://tfhub.dev) URL\n",
        "printed after the next cell execution.\n",
        "\n",
        "I've started with a Small BERT (with fewer parameters) since they are faster to fine-tune. If you like a small model but with higher accuracy, ALBERT might be next option. If you want even better accuracy, choose\n",
        "one of the classic BERT sizes or their recent refinements like Electra, Talking Heads, or a BERT Expert.\n",
        "\n",
        "Aside from the models available below, there are [multiple versions](https://tfhub.dev/google/collections/transformer_encoders_text/1) of the models that are larger and can yield even better accuracy, but they are too big to be fine-tuned on a single GPU. You will be able to do that on the [Solve GLUE tasks using BERT on a TPU colab](https://www.tensorflow.org/text/tutorials/bert_glue).\n",
        "\n",
        "You'll see in the code below that switching the tfhub.dev URL is enough to try any of these models, because all the differences between them are encapsulated in the SavedModels from TF Hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8_ctG55-uTX",
        "outputId": "e31de741-cbe0-4e1e-a536-01185e07528e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
            "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
          ]
        }
      ],
      "source": [
        "#@title Choose a BERT model to fine-tune\n",
        "\n",
        "bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8'  #@param [\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\"]\n",
        "\n",
        "map_name_to_handle = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/google/electra_small/2',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/google/electra_base/2',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
        "}\n",
        "\n",
        "map_model_to_preprocess = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "}\n",
        "\n",
        "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
        "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
        "\n",
        "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
        "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WrcxxTRDdHi"
      },
      "source": [
        "## The preprocessing model\n",
        "\n",
        "Text inputs need to be transformed to numeric token ids and arranged in several Tensors before being input to BERT. TensorFlow Hub provides a matching preprocessing model for each of the BERT models discussed above, which implements this transformation using TF ops from the TF.text library. It is not necessary to run pure Python code outside your TensorFlow model to preprocess text.\n",
        "\n",
        "The preprocessing model must be the one referenced by the documentation of the BERT model, which you can read at the URL printed above. For BERT models from the drop-down above, the preprocessing model is selected automatically.\n",
        "\n",
        "Note: we will load the preprocessing model into a [hub.KerasLayer](https://www.tensorflow.org/hub/api_docs/python/hub/KerasLayer) to compose your fine-tuned model. This is the preferred API to load a TF2-style SavedModel from TF Hub into a Keras model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SQi-jWd_jzq"
      },
      "outputs": [],
      "source": [
        "bert_preprocess_model = keras.Layer(tfhub_handle_preprocess)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4naBiEE_cZX"
      },
      "source": [
        "Let's try the preprocessing model on some text and see the output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9-zCzJpnuwS",
        "outputId": "560edce4-4619-4e9e-ea68-815907dc37c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keys       : ['input_type_ids', 'input_word_ids', 'input_mask']\n",
            "Shape      : (1, 128)\n",
            "Word Ids   : [ 101 2023 2003 2107 2019 6429 3185  999  102    0    0    0]\n",
            "Input Mask : [1 1 1 1 1 1 1 1 1 0 0 0]\n",
            "Type Ids   : [0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "text_test = ['this is such an amazing movie!']\n",
        "text_preprocessed = bert_preprocess_model(text_test)\n",
        "\n",
        "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
        "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
        "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
        "print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n",
        "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqL7ihkN_862"
      },
      "source": [
        "As you can see, now you have the 3 outputs from the preprocessing that a BERT model would use (`input_words_id`, `input_mask` and `input_type_ids`).\n",
        "\n",
        "Some other important points:\n",
        "- The input is truncated to 128 tokens. The number of tokens can be customized\n",
        "- The `input_type_ids` only have one value (0) because this is a single sentence input. For a multiple sentence input, it would have one number for each input.\n",
        "\n",
        "Since this text preprocessor is a TensorFlow model, It can be included in your model directly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKnLPSEmtp9i"
      },
      "source": [
        "## Using the BERT model\n",
        "\n",
        "Before putting BERT into your own model, let's take a look at its outputs. You will load it from TF Hub and see the returned values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXxYpK8ixL34"
      },
      "outputs": [],
      "source": [
        "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OoF9mebuSZc",
        "outputId": "4be4214e-f05b-4407-f09f-c9602eca911f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded BERT: https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
            "Pooled Outputs Shape:(1, 512)\n",
            "Pooled Outputs Values:[ 0.762629    0.99280983 -0.18611868  0.36673862  0.15233733  0.6550447\n",
            "  0.9681154  -0.9486271   0.00216128 -0.9877732   0.06842692 -0.97630584]\n",
            "Sequence Outputs Shape:(1, 128, 512)\n",
            "Sequence Outputs Values:[[-0.28946346  0.3432128   0.33231518 ...  0.21300825  0.7102068\n",
            "  -0.05771117]\n",
            " [-0.28742072  0.31981036 -0.23018576 ...  0.58455    -0.21329743\n",
            "   0.72692114]\n",
            " [-0.66157067  0.68876773 -0.8743301  ...  0.1087725  -0.26173177\n",
            "   0.47855407]\n",
            " ...\n",
            " [-0.2256118  -0.2892561  -0.0706445  ...  0.47566038  0.83277136\n",
            "   0.40025333]\n",
            " [-0.2982428  -0.27473134 -0.05450517 ...  0.48849747  1.0955354\n",
            "   0.18163396]\n",
            " [-0.44378242  0.00930811  0.07223688 ...  0.1729009   1.1833243\n",
            "   0.07898017]]\n"
          ]
        }
      ],
      "source": [
        "bert_results = bert_model(text_preprocessed)\n",
        "\n",
        "print(f'Loaded BERT: {tfhub_handle_encoder}')\n",
        "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
        "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n",
        "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
        "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYvPOz_nV1KW",
        "outputId": "fd4d2551-39b3-450b-a2a7-40bf92604512"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keys       : ['sequence_output', 'pooled_output', 'encoder_outputs', 'default']\n"
          ]
        }
      ],
      "source": [
        "print(f'Keys       : {list(bert_results.keys())}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sm61jDrezAll"
      },
      "source": [
        "The BERT models return a map with 3 important keys: `pooled_output`, `sequence_output`, `encoder_outputs`:\n",
        "\n",
        "- `pooled_output` represents each input sequence as a whole. The shape is `[batch_size, H]`. You can think of this as an embedding for the entire movie review.\n",
        "- `sequence_output` represents each input token in the context. The shape is `[batch_size, seq_length, H]`. You can think of this as a contextual embedding for every token in the movie review.\n",
        "- `encoder_outputs` are the intermediate activations of the `L` Transformer blocks. `outputs[\"encoder_outputs\"][i]` is a Tensor of shape `[batch_size, seq_length, 1024]` with the outputs of the i-th Transformer block, for `0 <= i < L`. The last value of the list is equal to `sequence_output`.\n",
        "\n",
        "For the fine-tuning you are going to use the `pooled_output` array."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDNKfAXbDnJH"
      },
      "source": [
        "## Define your model\n",
        "\n",
        "You will create a very simple fine-tuned model, with the preprocessing model, the selected BERT model, one Dense and a Dropout layer.\n",
        "\n",
        "Note: for more information about the base model's input and output you can follow the model's URL for documentation. Here specifically, you don't need to worry about it because the preprocessing model will take care of that for you.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aksj743St9ga"
      },
      "outputs": [],
      "source": [
        "def build_classifier_model():\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  net = outputs['pooled_output']\n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs4yhFraBuGQ"
      },
      "source": [
        "Let's check that the model runs with the output of the preprocessing model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGMF8AZcB2Zy",
        "outputId": "0408ab5b-0828-488b-d1f4-1fb45b748bf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor([[0.53137827]], shape=(1, 1), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "classifier_model = build_classifier_model()\n",
        "bert_raw_result = classifier_model(tf.constant(text_test))\n",
        "print(tf.sigmoid(bert_raw_result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTUzNV2JE2G3"
      },
      "source": [
        "The output is meaningless, of course, because the model has not been trained yet.\n",
        "\n",
        "Let's take a look at the model's structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "0EmzyHZXKIpm",
        "outputId": "33b96d85-e692-4ceb-d76e-28bcba369020"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.keras.utils.plot_model(classifier_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbUWoZMwc302"
      },
      "source": [
        "## Model training\n",
        "\n",
        "You now have all the pieces to train a model, including the preprocessing module, BERT encoder, data, and classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpJ3xcwDT56v"
      },
      "source": [
        "### Loss function\n",
        "\n",
        "Since this is a binary classification problem and the model outputs a probability (a single-unit layer), you'll use `losses.BinaryCrossentropy` loss function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWPOZE-L3AgE"
      },
      "outputs": [],
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "metrics = tf.metrics.BinaryAccuracy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77psrpfzbxtp"
      },
      "source": [
        "### Optimizer\n",
        "\n",
        "For fine-tuning, let's use the same optimizer that BERT was originally trained with: the \"Adaptive Moments\" (Adam). This optimizer minimizes the prediction loss and does regularization by weight decay (not using moments), which is also known as [AdamW](https://arxiv.org/abs/1711.05101).\n",
        "\n",
        "For the learning rate (`init_lr`), you will use the same schedule as BERT pre-training: linear decay of a notional initial learning rate, prefixed with a linear warm-up phase over the first 10% of training steps (`num_warmup_steps`). In line with the BERT paper, the initial learning rate is smaller for fine-tuning (best of 5e-5, 3e-5, 2e-5)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9eP2y9dbw32"
      },
      "outputs": [],
      "source": [
        "epochs = 5\n",
        "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "\n",
        "init_lr = 3e-5\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqlarlpC_v0g"
      },
      "source": [
        "### Loading the BERT model and training\n",
        "\n",
        "Using the `classifier_model` you created earlier, you can compile the model with the loss, metric and optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7GPDhR98jsD"
      },
      "outputs": [],
      "source": [
        "classifier_model.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpBuV5j2cS_b"
      },
      "source": [
        "Note: training time will vary depending on the complexity of the BERT model you have selected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtfDFAnN_Neu",
        "outputId": "f9b44570-d9d3-4600-a804-303d8f08541f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
            "Epoch 1/5\n",
            "625/625 [==============================] - 7199s 11s/step - loss: 0.4835 - binary_accuracy: 0.7413 - val_loss: 0.3786 - val_binary_accuracy: 0.8148\n",
            "Epoch 2/5\n",
            "625/625 [==============================] - 7091s 11s/step - loss: 0.3269 - binary_accuracy: 0.8563 - val_loss: 0.3805 - val_binary_accuracy: 0.8320\n",
            "Epoch 3/5\n",
            "  7/625 [..............................] - ETA: 1:48:20 - loss: 0.3499 - binary_accuracy: 0.8438"
          ]
        }
      ],
      "source": [
        "print(f'Training model with {tfhub_handle_encoder}')\n",
        "history = classifier_model.fit(x=train_ds,\n",
        "                               validation_data=val_ds,\n",
        "                               epochs=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBthMlTSV8kn"
      },
      "source": [
        "### Evaluate the model\n",
        "\n",
        "Let's see how the model performs. Two values will be returned. Loss (a number which represents the error, lower values are better), and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slqB-urBV9sP"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = classifier_model.evaluate(test_ds)\n",
        "\n",
        "print(f'Loss: {loss}')\n",
        "print(f'Accuracy: {accuracy}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uttWpgmSfzq9"
      },
      "source": [
        "### Plot the accuracy and loss over time\n",
        "\n",
        "Based on the `History` object returned by `model.fit()`. You can plot the training and validation loss for comparison, as well as the training and validation accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiythcODf0xo"
      },
      "outputs": [],
      "source": [
        "history_dict = history.history\n",
        "print(history_dict.keys())\n",
        "\n",
        "acc = history_dict['binary_accuracy']\n",
        "val_acc = history_dict['val_binary_accuracy']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.subplot(2, 1, 1)\n",
        "# r is for \"solid red line\"\n",
        "plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "# plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(epochs, acc, 'r', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzJZCo-cf-Jf"
      },
      "source": [
        "In this plot, the red lines represent the training loss and accuracy, and the blue lines are the validation loss and accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rtn7jewb6dg4"
      },
      "source": [
        "## Export for inference\n",
        "\n",
        "Now you just save your fine-tuned model for later use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShcvqJAgVera"
      },
      "outputs": [],
      "source": [
        "dataset_name = 'imdb'\n",
        "saved_model_path = './{}_bert'.format(dataset_name.replace('/', '_'))\n",
        "\n",
        "classifier_model.save(saved_model_path, include_optimizer=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbI25bS1vD7s"
      },
      "source": [
        "Let's reload the model, so you can try it side by side with the model that is still in memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUEWVskZjEF0"
      },
      "outputs": [],
      "source": [
        "reloaded_model = tf.saved_model.load(saved_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyTappHTvNCz"
      },
      "source": [
        "Here you can test your model on any sentence you want, just add to the examples variable below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBWzH6exlCPS"
      },
      "outputs": [],
      "source": [
        "def print_my_examples(inputs, results):\n",
        "  result_for_printing = \\\n",
        "    [f'input: {inputs[i]:<30} : score: {results[i][0]:.6f}'\n",
        "                         for i in range(len(inputs))]\n",
        "  print(*result_for_printing, sep='\\n')\n",
        "  print()\n",
        "\n",
        "\n",
        "examples = [\n",
        "    'this is such an amazing movie!',  # this is the same sentence tried earlier\n",
        "    'The movie was great!',\n",
        "    'The movie was meh.',\n",
        "    'The movie was okish.',\n",
        "    'The movie was terrible...'\n",
        "]\n",
        "\n",
        "reloaded_results = tf.sigmoid(reloaded_model(tf.constant(examples)))\n",
        "original_results = tf.sigmoid(classifier_model(tf.constant(examples)))\n",
        "\n",
        "print('Results from the saved model:')\n",
        "print_my_examples(examples, reloaded_results)\n",
        "print('Results from the model in memory:')\n",
        "print_my_examples(examples, original_results)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}